# Google Quest Q&A Labeling - æŠ€è¡“æ–¹æ¡ˆæ–‡æª”

## ğŸ“‹ ç›®éŒ„
1. [å•é¡Œå®šç¾©](#1-å•é¡Œå®šç¾©)
2. [ç•¶å‰è§£æ±ºæ–¹æ¡ˆ](#2-ç•¶å‰è§£æ±ºæ–¹æ¡ˆ)
3. [æœªä¾†å„ªåŒ–æ–¹å‘](#3-æœªä¾†å„ªåŒ–æ–¹å‘)

---

## 1. å•é¡Œå®šç¾©

### 1.1 ç«¶è³½èƒŒæ™¯

**Google Quest Q&A Labeling** æ˜¯ä¸€å€‹å¤šæ¨™ç±¤å›æ­¸å•é¡Œï¼Œç›®æ¨™æ˜¯é æ¸¬ Q&A é…å°çš„ 30 å€‹å“è³ªæŒ‡æ¨™ã€‚

| é …ç›® | èªªæ˜ |
|------|------|
| **ä»»å‹™é¡å‹** | å¤šæ¨™ç±¤å›æ­¸ (Multi-label Regression) |
| **ç›®æ¨™æ•¸é‡** | 30 å€‹é€£çºŒæ¨™ç±¤ (ç¯„åœ 0-1) |
| **è©•ä¼°æŒ‡æ¨™** | Mean Spearman Correlation (è·¨ 30 å€‹æ¨™ç±¤å–å¹³å‡) |
| **æ•¸æ“šè¦æ¨¡** | è¨“ç·´é›† ~6,079 ç­†ï¼Œæ¸¬è©¦é›† ~476 ç­† |

### 1.2 ç›®æ¨™æ¨™ç±¤åˆ†é¡

```
Question ç›¸é—œ (21 å€‹):
â”œâ”€â”€ æ„åœ–ç†è§£: question_asker_intent_understanding, question_body_critical, ...
â”œâ”€â”€ å•é¡Œé¡å‹: question_type_compare, question_type_definition, question_type_entity, ...
â””â”€â”€ å“è³ªè©•ä¼°: question_well_written, question_interestingness_others, ...

Answer ç›¸é—œ (9 å€‹):
â”œâ”€â”€ å›ç­”å“è³ª: answer_helpful, answer_plausible, answer_relevance, ...
â””â”€â”€ å›ç­”é¡å‹: answer_type_instructions, answer_type_procedure, ...
```

### 1.3 æ ¸å¿ƒæŒ‘æˆ°

| æŒ‘æˆ° | æè¿° | å½±éŸ¿ |
|------|------|------|
| **å°æ•¸æ“šé›†** | åƒ… ~6,000 ç­†è¨“ç·´æ•¸æ“š | å®¹æ˜“éæ“¬åˆï¼Œéœ€è¦å¼·æ­£å‰‡åŒ– |
| **æ¨™ç±¤ä¸å¹³è¡¡** | æŸäº›æ¨™ç±¤æ¥µåº¦ç¨€ç–ï¼ˆå¦‚ `question_type_spelling`ï¼‰ | é›£ä»¥å­¸ç¿’ç¨€ç–æ¨™ç±¤çš„åˆ†å¸ƒ |
| **æ’åè©•ä¼°** | Spearman åªé—œå¿ƒæ’åï¼Œä¸é—œå¿ƒçµ•å°å€¼ | éœ€è¦é‡å°æ’åå„ªåŒ–çš„å¾Œè™•ç† |
| **ç¾¤çµ„ç›¸é—œæ€§** | åŒä¸€å•é¡Œå¯èƒ½æœ‰å¤šå€‹ç­”æ¡ˆ | éœ€è¦ GroupKFold é˜²æ­¢æ•¸æ“šæ´©æ¼ |

---

## 2. ç•¶å‰è§£æ±ºæ–¹æ¡ˆ

### 2.1 æ¨¡å‹æ¶æ§‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    QuestDebertaModel                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input: [CLS] question_title [SEP] question_body [SEP]      â”‚
â”‚         answer [SEP]                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚           DeBERTa-v3-base/large                      â”‚   â”‚
â”‚  â”‚           (output_hidden_states=True)                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         Weighted Layer Pooling                       â”‚   â”‚
â”‚  â”‚   [CLS]â‚€, [CLS]â‚, ..., [CLS]â‚â‚‚ â†’ Weighted Sum       â”‚   â”‚
â”‚  â”‚   weights = softmax(learnable_params)                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         Multi-Sample Dropout (5x)                    â”‚   â”‚
â”‚  â”‚   dropoutâ‚(x), dropoutâ‚‚(x), ..., dropoutâ‚…(x)        â”‚   â”‚
â”‚  â”‚   â†’ Average all outputs                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â†“                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         Linear(hidden_size, 30) + Sigmoid            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â”‚  Output: 30 probability scores âˆˆ [0, 1]                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 é—œéµæŠ€è¡“ç´°ç¯€

#### A. Weighted Layer Pooling
è‡ªå‹•å­¸ç¿’æ¯å€‹ Transformer å±¤çš„æ¬Šé‡ï¼Œå°æ‰€æœ‰å±¤çš„ `[CLS]` token é€²è¡ŒåŠ æ¬Šæ±‚å’Œã€‚åˆå§‹åŒ–æ™‚åå‘æœ€å¾Œå¹¾å±¤ï¼Œä½†è¨“ç·´éç¨‹ä¸­æœƒè‡ªå‹•èª¿æ•´ä»¥é©æ‡‰ä»»å‹™éœ€æ±‚ã€‚

**å„ªé»**: è‡ªå‹•ç™¼ç¾å“ªäº›å±¤çš„è¡¨ç¤ºæœ€é©åˆæ­¤ä»»å‹™

#### B. Multi-Sample Dropout (5x)
ä½¿ç”¨ 5 å€‹ä¸åŒçš„ dropout å±¤å°ç›¸åŒè¼¸å…¥é€²è¡Œå¤šæ¬¡å‰å‘å‚³æ’­ï¼Œæœ€å¾Œå–å¹³å‡ä½œç‚ºæœ€çµ‚è¼¸å‡ºã€‚

**å„ªé»**: 
- è¨“ç·´æ™‚ç­‰æ•ˆæ–¼ 5 å€‹æ¨¡å‹çš„ ensemble
- å¢åŠ æ­£å‰‡åŒ–ï¼Œæ¸›å°‘éæ“¬åˆ
- æ¨ç†æ™‚ä¿æŒ dropout æ•ˆæœçš„å¹³å‡

#### C. å·®ç•°å­¸ç¿’ç‡ (Differential Learning Rate)
Backbone ä½¿ç”¨è¼ƒå°å­¸ç¿’ç‡ï¼ˆ1e-5ï¼‰ï¼ŒClassification Head ä½¿ç”¨è¼ƒå¤§å­¸ç¿’ç‡ï¼ˆ5e-5ï¼‰ã€‚

**åŸå› **: é è¨“ç·´çš„ backbone å·²ç¶“å­¸åˆ°è‰¯å¥½è¡¨ç¤ºï¼Œåªéœ€å¾®èª¿ï¼›æ–°åˆå§‹åŒ–çš„ head éœ€è¦è¼ƒå¿«å­¸ç¿’

### 2.3 è¨“ç·´ç­–ç•¥

| ç­–ç•¥ | é…ç½® | èªªæ˜ |
|------|------|------|
| **äº¤å‰é©—è­‰** | GroupKFold (5-fold) | æŒ‰ `question_title` åˆ†çµ„ï¼Œé˜²æ­¢åŒå•é¡Œæ´©æ¼ |
| **æå¤±å‡½æ•¸** | BCEWithLogitsLoss | æ•¸å€¼ç©©å®šæ€§å„ªæ–¼ BCELoss |
| **å„ªåŒ–å™¨** | AdamW (weight_decay=0.01) | å¸¶æ¬Šé‡è¡°æ¸›çš„ Adam |
| **å­¸ç¿’ç‡èª¿åº¦** | Linear warmup + decay | 10% warmup steps |
| **æ¢¯åº¦è£å‰ª** | max_norm=1.0 | é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ |
| **æ—©åœ** | patience=3 | é˜²æ­¢éæ“¬åˆ |

### 2.4 å¾Œè™•ç†ï¼šDistribution Matching

é€™æ˜¯**å† è»æ–¹æ¡ˆ**çš„æ ¸å¿ƒæŠ€å·§ï¼Œé‡å° Spearman æ’åç›¸é—œæ€§å„ªåŒ–ã€‚

**å·¥ä½œåŸç†**:
1. æŒ‰é æ¸¬å€¼å°æ¸¬è©¦æ¨£æœ¬æ’åº
2. æ ¹æ“šè¨“ç·´é›†ä¸­æ¯å€‹å€¼çš„æ¯”ä¾‹ï¼Œåœ¨æ¸¬è©¦é›†ä¸­åˆ†é…ç›¸æ‡‰æ•¸é‡çš„æ¨£æœ¬
3. ç›¸åŒæ¯”ä¾‹å€é–“çš„æ¨£æœ¬ç²å¾—ç›¸åŒçš„åˆ†æ•¸

**æ‡‰ç”¨ç­–ç•¥**:
- ç•¶å‰æ‡‰ç”¨æ–¼ **17 å€‹æ¬„ä½**ï¼ˆç¨€ç–åˆ†å¸ƒæˆ–é›¢æ•£å€¼çš„æ¬„ä½ï¼‰
- åŒ…æ‹¬ï¼š`question_conversational`, `question_type_compare`, `question_type_definition`, `question_type_entity`, `question_has_commonly_accepted_answer`, ç­‰

**æ•ˆæœ**: å† è»å ±å‘Š +0.027~0.030 çš„æå‡

### 2.5 æ¨ç†æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    5-Fold Ensemble                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Modelâ‚ â”€â”€â†’ Predictionsâ‚ â”€â”                               â”‚
â”‚   Modelâ‚‚ â”€â”€â†’ Predictionsâ‚‚ â”€â”¼â”€â”€â†’ Average â”€â”€â†’ Ensemble Pred  â”‚
â”‚   Modelâ‚ƒ â”€â”€â†’ Predictionsâ‚ƒ â”€â”¤                               â”‚
â”‚   Modelâ‚„ â”€â”€â†’ Predictionsâ‚„ â”€â”¤                               â”‚
â”‚   Modelâ‚… â”€â”€â†’ Predictionsâ‚… â”€â”˜                               â”‚
â”‚                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   Ensemble Pred â”€â”€â†’ Distribution Matching â”€â”€â†’ Final Pred   â”‚
â”‚                     (17 selected columns)                   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. æœªä¾†å„ªåŒ–æ–¹å‘

### ğŸ¯ å„ªåŒ–ç­–ç•¥ç¸½è¦½

åŸºæ–¼ã€Œ**æœ€å°åŠ›æ°£ï¼Œæœ€å¤§æ”¶ç›Š**ã€åŸå‰‡ï¼Œä»¥ä¸‹æŒ‰ç…§**å¯¦ç¾é›£åº¦**å’Œ**é æœŸæ•ˆæœ**åˆ†é¡ï¼š

```
ç«‹å³å¯åš (1-2 å°æ™‚):
â”œâ”€â”€ âœ… EMA (Exponential Moving Average)
â”œâ”€â”€ âœ… Layer-wise LR Decay
â””â”€â”€ âœ… Ranking Loss (ä½œç‚ºè¼”åŠ© loss)

åŠå¤©å·¥ä½œé‡:
â”œâ”€â”€ ğŸ”„ AWP (Adversarial Weight Perturbation)
â”œâ”€â”€ ğŸ”„ ModernBERT ä½œç‚º backbone
â””â”€â”€ ğŸ”„ å„ªåŒ– Distribution Matching æ¬„ä½

å¯é¸å¯¦é©— (éœ€è¦é©—è­‰):
â”œâ”€â”€ ğŸ“Š çµæ§‹åŒ–è¼¸å…¥æ ¼å¼
â”œâ”€â”€ ğŸ“Š é›™å¡”æ¶æ§‹
â””â”€â”€ ğŸ“Š æ¨¡å‹èåˆ
```

---

### 3.1 æ¨¡å‹å±¤é¢

#### A. Backbone æ¨¡å‹é¸æ“‡

**ğŸ” æ¶æ§‹é©ç”¨æ€§åˆ†æ**

| æ¶æ§‹é¡å‹ | ä»£è¡¨æ¨¡å‹ | é©ç”¨æ€§ | èªªæ˜ |
|----------|----------|--------|------|
| **Encoder-only** | DeBERTa-v3, ModernBERT | â­â­â­ æœ€é©åˆ | å°ˆç‚ºç†è§£ä»»å‹™è¨­è¨ˆ |
| **Encoder-Decoder** | T5, BART, Flan-T5 | â­â­ å¯ç”¨ | ç”Ÿæˆèƒ½åŠ›åœ¨æ­¤ä»»å‹™æµªè²» |
| **Decoder-only** | Llama, Mistral, Qwen | â­ ä¸å»ºè­° | å›æ­¸ä»»å‹™éœ€é¡å¤–è¨­è¨ˆ |

**ğŸ”¥ 2024-2025 æ¨è–¦æ¨¡å‹**

| æ¨¡å‹ | åƒæ•¸é‡ | Context Length | é æœŸæ•ˆæœ | å‚™è¨» |
|------|--------|----------------|----------|------|
| `deberta-v3-large` | 304M | 512 | +0.01~0.02 | âœ… ç•¶å‰ä½¿ç”¨ |
| `answerdotai/ModernBERT-large` | 395M | 8192 | +0.015~0.025 | ğŸ”¥ 2024 æ–°æ¶æ§‹ |
| `Alibaba-NLP/gte-Qwen2-1.5B-instruct` | 1.5B | 32K | +0.02~0.03 | Embedding å°ˆç”¨ |
| `deberta-v2-xlarge` | 900M | 512 | +0.02~0.03 | éœ€è¦ A100 GPU |

**âš ï¸ é‡è¦è€ƒé‡**

é€™å€‹ç«¶è³½æœ‰ç‰¹æ®Šæ€§ï¼š
- æ¸¬è©¦é›†åƒ… **~476 ç­†**
- è©•ä¼°æŒ‡æ¨™æ˜¯ **Spearman correlation**ï¼ˆæ’åç›¸é—œï¼‰
- Distribution matching æœƒé‡æ–°åˆ†é…æ’å

é€™æ„å‘³è‘—ï¼š
1. **æ›´å¤§æ¨¡å‹ä¸ä¸€å®šæ›´å¥½** â€” å°æ•¸æ“š + æ’åè©•ä¼° = å®¹æ˜“éæ“¬åˆ
2. **æ¨¡å‹å·®ç•°å¯èƒ½è¢«å¾Œè™•ç†æŠ¹å¹³** â€” å¾Œè™•ç†å°æ’åçš„å½±éŸ¿å¾ˆå¤§

ğŸ’¡ **å»ºè­°å„ªå…ˆç´š**ï¼š
```
1. ModernBERT-large (æ–°æ¶æ§‹ + æ›´é•· context)
2. ä¿æŒ deberta-v3-large + å„ªåŒ–è¨“ç·´æŠ€å·§
3. Qwen2-1.5B (å¦‚æœè³‡æºå……è¶³)
```

#### B. æ¨¡å‹èåˆ (Model Blending)

èåˆä¸åŒæ¶æ§‹æ¨¡å‹çš„é æ¸¬çµæœï¼Œé€šéåŠ æ¬Šå¹³å‡æå‡ç©©å®šæ€§ã€‚å€™é¸æ¨¡å‹åŒ…æ‹¬ DeBERTa-v3-base/largeã€RoBERTa-largeã€ALBERT-xxlarge-v2 ç­‰ã€‚æ¬Šé‡å¯é€šéé©—è­‰é›†å„ªåŒ–ç¢ºå®šã€‚

**é æœŸæ•ˆæœ**: +0.01~0.02

#### C. çŸ¥è­˜è’¸é¤¾ (Knowledge Distillation)

ä½¿ç”¨å¤§æ¨¡å‹ï¼ˆå¦‚ deberta-v2-xxlargeï¼‰ä½œç‚º Teacher ç”Ÿæˆè»Ÿæ¨™ç±¤ï¼Œè¨“ç·´å°æ¨¡å‹ï¼ˆå¦‚ deberta-v3-baseï¼‰ä½œç‚º Studentã€‚å¯åœ¨ä¿æŒç²¾åº¦çš„åŒæ™‚åŠ å¿«æ¨ç†é€Ÿåº¦ã€‚

**é©ç”¨å ´æ™¯**: éœ€è¦åœ¨æ¨ç†é€Ÿåº¦å’Œç²¾åº¦ä¹‹é–“å–å¾—å¹³è¡¡æ™‚

### 3.2 æ•¸æ“šå±¤é¢

#### A. æ”¹é€² Input Format

**ç•¶å‰æ ¼å¼**: `[CLS] question_title [SEP] question_body [SEP] answer [SEP]`

**æ½›åœ¨å•é¡Œ**:
1. æ²’æœ‰æ˜ç¢ºå€åˆ† Question vs Answer çš„èªç¾©é‚Šç•Œ
2. æœªåˆ©ç”¨ `category` å’Œ `host` ç­‰å…ƒä¿¡æ¯

**æ”¹é€²æ–¹æ¡ˆ**:
- **æ–¹æ¡ˆ 1 - çµæ§‹åŒ–æç¤º**: ä½¿ç”¨è‡ªç„¶èªè¨€æ¨™è¨˜ï¼ˆå¦‚ "Question Title:", "Answer:"ï¼‰æ˜ç¢ºæ¨™ç¤ºå„éƒ¨åˆ†
- **æ–¹æ¡ˆ 2 - é›™å¡”æ¶æ§‹**: åˆ†åˆ¥ç·¨ç¢¼ Question å’Œ Answerï¼Œå†é€šé Cross-attention èåˆ

**âš ï¸ æ³¨æ„**: é€™æ˜¯ 6 å¹´å‰çš„ç«¶è³½ï¼Œç°¡å–®æ–¹æ³•å¯èƒ½å·²è¢«å……åˆ†æ¢ç´¢ã€‚æ”¹è®Š input format çš„é æœŸæ”¶ç›Šæœ‰é™ï¼ˆ+0.003~0.008ï¼‰ã€‚

#### B. æ•¸æ“šå¢å¼·ï¼ˆè¬¹æ…ä½¿ç”¨ï¼‰

| æŠ€è¡“ | é æœŸæ•ˆæœ | é¢¨éšª |
|------|----------|------|
| **Mixup (Embeddingå±¤)** | +0.005~0.01 | å¯èƒ½ç ´å£æ’åé—œä¿‚ |
| **Back Translation** | +0.002~0.005 | è¨ˆç®—æˆæœ¬é«˜ |
| **Cutout (Tokené®è”½)** | æ­£å‰‡åŒ–æ•ˆæœ | å¯èƒ½ä¸Ÿå¤±é—œéµä¿¡æ¯ |

**ğŸ’¡ å»ºè­°**: æ•¸æ“šå¢å¼·åœ¨å°æ•¸æ“šé›†ï¼ˆ~6Kï¼‰ä¸Šæ•ˆæœæœ‰é™ï¼Œä¸ä½œç‚ºå„ªå…ˆé …ã€‚

### 3.3 è¨“ç·´ç­–ç•¥å„ªåŒ–

#### A. ğŸ”¥ æå¤±å‡½æ•¸å„ªåŒ–ï¼ˆé«˜å„ªå…ˆç´šï¼‰

**æ ¸å¿ƒå•é¡Œ**: BCELoss å„ªåŒ–ã€Œé æ¸¬å€¼æ¥è¿‘çœŸå¯¦å€¼ã€ï¼Œä½† Spearman åªé—œå¿ƒã€Œæ’åé †åºã€

| Loss é¡å‹ | å„ªåŒ–ç›®æ¨™ | èˆ‡ Spearman å°é½Šåº¦ | æ¨è–¦åº¦ |
|----------|----------|-------------------|--------|
| BCELoss | æ•¸å€¼æ¥è¿‘ | âŒ ä¸å°é½Š | ç•¶å‰ä½¿ç”¨ |
| **Pairwise Ranking Loss** | æ’åé †åº | âœ… å®Œå…¨å°é½Š | â­â­â­â­â­ |
| **Soft Spearman Loss** | æ’åç›¸é—œæ€§ | âœ… å®Œå…¨å°é½Š | â­â­â­â­ |
| Combined Loss | æ•¸å€¼ + æ’å | âš ï¸ éƒ¨åˆ†å°é½Š | â­â­â­â­ |

**ğŸ’¡ æ¨è–¦æ–¹æ¡ˆ**

**1. Pairwise Ranking Loss**
- å°æ¯å°æ¨£æœ¬ (i, j)ï¼Œå¦‚æœ target[i] > target[j]ï¼Œå‰‡å¸Œæœ› pred[i] > pred[j]
- ä½¿ç”¨ Margin Ranking Loss ç¢ºä¿æ­£ç¢ºçš„ç›¸å°æ’åº
- åªè€ƒæ…® target æœ‰æ˜é¡¯å·®ç•°çš„æ¨£æœ¬å°ï¼ˆé–¾å€¼ > 0.05ï¼‰

**2. Soft Spearman Loss**
- ä½¿ç”¨ soft rankingï¼ˆsigmoid è¿‘ä¼¼ï¼‰è¨ˆç®—å¯å¾®åˆ†çš„æ’å
- è¨ˆç®—é æ¸¬æ’åå’ŒçœŸå¯¦æ’åçš„ Pearson correlation
- Loss = 1 - mean correlation

**3. Combined Lossï¼ˆæ¨è–¦ï¼‰**
- çµåˆ BCEã€Ranking å’Œ Spearman Loss
- æ¬Šé‡å»ºè­°ï¼šÎ±=0.5 (BCE), Î²=0.3 (Ranking), 0.2 (Spearman)

**âš ï¸ æ³¨æ„äº‹é …**:
- Ranking loss è¨ˆç®—è¤‡é›œåº¦æ˜¯ O(NÂ²)ï¼Œbatch size å»ºè­° â‰¤16
- éœ€è¦èª¿æ•´ loss æ¬Šé‡ä»¥å¹³è¡¡å„é …
- é æœŸæå‡ï¼š+0.005~0.015

#### B. ğŸ”¥ ç¾ä»£ Kaggle å„ªåŒ–æŠ€å·§ï¼ˆé«˜æ€§åƒ¹æ¯”ï¼‰

**1. EMA (Exponential Moving Average)** â€” ç«‹å³å¯åš âœ…

ç¶­è­·æ¨¡å‹æ¬Šé‡çš„æŒ‡æ•¸ç§»å‹•å¹³å‡ï¼Œæ¨ç†æ™‚ä½¿ç”¨å¹³æ»‘å¾Œçš„æ¬Šé‡ã€‚

**å¯¦ç¾è¦é»**:
- è¨“ç·´æ™‚ï¼šæ¯å€‹ step å¾Œæ›´æ–° shadow weights (decay=0.9995)
- æ¨ç†æ™‚ï¼šè‡¨æ™‚æ›¿æ›ç‚º shadow weightsï¼Œé æ¸¬å¾Œæ¢å¾©

**é æœŸæ•ˆæœ**: +0.002~0.008

**2. Layer-wise LR Decay (LLRD)** â€” ç«‹å³å¯åš âœ…

ç‚º Transformer çš„ä¸åŒå±¤è¨­ç½®éæ¸›çš„å­¸ç¿’ç‡ï¼Œè¶Šé è¿‘è¼¸å…¥å±¤å­¸ç¿’ç‡è¶Šå°ã€‚

**å¯¦ç¾è¦é»**:
- Embedding å±¤ï¼šæœ€å°å­¸ç¿’ç‡ï¼ˆbase_lr Ã— decay^n_layersï¼‰
- Encoder å±¤ï¼šé€å±¤éå¢ï¼ˆbase_lr Ã— decay^(n_layers - layer_i)ï¼‰
- Head å±¤ï¼šæœ€å¤§å­¸ç¿’ç‡ï¼ˆbase_lr Ã— 5ï¼‰
- æ¨è–¦ decay=0.9

**é æœŸæ•ˆæœ**: +0.003~0.008

**3. AWP (Adversarial Weight Perturbation)** â€” åŠå¤©å·¥ä½œé‡ ğŸ”„

å°æ¨¡å‹æ¬Šé‡æ·»åŠ å°æŠ—æ“¾å‹•ä»¥æé«˜æ³›åŒ–èƒ½åŠ›ï¼ˆ2020å¹´å¾Œ Kaggle NLP æ¯”è³½çš„æ¨™æº–æŠ€å·§ï¼‰ã€‚

**å¯¦ç¾è¦é»**:
- åœ¨æ­£å¸¸è¨“ç·´æ­¥é©Ÿå¾Œï¼Œæ²¿æ¢¯åº¦æ–¹å‘æ“¾å‹•æ¬Šé‡
- ä½¿ç”¨æ“¾å‹•å¾Œçš„æ¨¡å‹è¨ˆç®—å°æŠ—æå¤±ä¸¦åå‘å‚³æ’­
- æ¢å¾©åŸå§‹æ¬Šé‡ä¸¦æ›´æ–°
- å»ºè­°æ¯ 2-4 å€‹ step åŸ·è¡Œä¸€æ¬¡ï¼Œé¿å…è¨ˆç®—é–‹éŠ·éå¤§

**é æœŸæ•ˆæœ**: +0.005~0.015

#### C. å­¸ç¿’ç‡èª¿åº¦

æ¨è–¦ä½¿ç”¨ **Cosine Annealing with Warm Restarts**ï¼Œæ¯å€‹ epoch é‡å•Ÿå­¸ç¿’ç‡ä¸¦é€æ¼¸æ“´å¤§é€±æœŸï¼ˆT_mult=2ï¼‰ï¼Œæœ€å°å­¸ç¿’ç‡è¨­ç‚º 1e-7ã€‚

### 3.4 å¾Œè™•ç†å„ªåŒ–

#### A. å„ªåŒ– Distribution Matching æ¬„ä½é¸æ“‡ï¼ˆé«˜å„ªå…ˆç´šï¼‰

**ç•¶å‰å•é¡Œ**: å›ºå®šæ‡‰ç”¨æ–¼ 17 å€‹æ¬„ä½ï¼Œä½†ä¸æ˜¯æ‰€æœ‰æ¬„ä½éƒ½å—ç›Š

**æ”¹é€²æ–¹æ¡ˆ**: é€šé OOF é æ¸¬è‡ªå‹•é¸æ“‡æœ€ä½³æ¬„ä½

**å¯¦ç¾æ­¥é©Ÿ**:
1. å°æ¯å€‹æ¬„ä½ï¼Œè¨ˆç®—æ‡‰ç”¨ DM å‰å¾Œçš„ CV Spearman score
2. åªä¿ç•™æœ‰æ˜é¡¯æå‡ï¼ˆimprovement > 0.001ï¼‰çš„æ¬„ä½
3. æŒ‰æå‡å¹…åº¦æ’åºï¼Œé¸æ“‡ Top-K æ¬„ä½

**é æœŸæ•ˆæœ**: ç•¶å‰å›ºå®š 17 å€‹æ¬„ä½ â†’ è‡ªå‹•é¸æ“‡æœ€å„ªæ¬„ä½ï¼Œ+0.003~0.008

#### B. æ¸¬è©¦æ™‚å¢å¼· (TTA) - å¯é¸

å°åŒä¸€è¼¸å…¥ç”Ÿæˆå¤šå€‹è®Šé«”ï¼ˆé€šéä¸åŒéš¨æ©Ÿç¨®å­æˆ– dropoutï¼‰ï¼Œå–é æ¸¬å¹³å‡å€¼ã€‚

**æ³¨æ„**: ç•¶å‰æ¨¡å‹å·²æœ‰ multi-sample dropout (5x)ï¼ŒTTA çš„é¡å¤–æ”¶ç›Šå¯èƒ½æœ‰é™ï¼ˆ+0.001~0.003ï¼‰

---

### 3.5 ğŸ“Š å„ªåŒ–æ–¹å‘ç¸½çµèˆ‡å„ªå…ˆç´š

#### ğŸ¯ ç«‹å³å¯åšï¼ˆ1-2 å°æ™‚ï¼Œé«˜æ”¶ç›Šï¼‰

| æ–¹æ³• | é æœŸæå‡ | å¯¦ç¾é›£åº¦ | å„ªå…ˆç´š |
|------|----------|----------|--------|
| **EMA** | +0.002~0.008 | â­ å¾ˆä½ | ğŸ”´ æœ€é«˜ |
| **Layer-wise LR Decay** | +0.003~0.008 | â­ å¾ˆä½ | ğŸ”´ æœ€é«˜ |
| **Ranking Loss (è¼”åŠ©)** | +0.005~0.012 | â­â­ ä½ | ğŸ”´ æœ€é«˜ |
| **å„ªåŒ– DM æ¬„ä½é¸æ“‡** | +0.003~0.008 | â­ å¾ˆä½ | ğŸ”´ é«˜ |

**å¯¦æ–½é †åº**:
```bash
1. æ·»åŠ  EMA (30 åˆ†é˜)
2. æ·»åŠ  Layer-wise LR Decay (30 åˆ†é˜)
3. å¯¦é©— Ranking Loss (1 å°æ™‚)
4. å„ªåŒ– Distribution Matching æ¬„ä½ (30 åˆ†é˜)
```

#### ğŸ”§ åŠå¤©å·¥ä½œé‡ï¼ˆé«˜æ€§åƒ¹æ¯”ï¼‰

| æ–¹æ³• | é æœŸæå‡ | å¯¦ç¾é›£åº¦ | å„ªå…ˆç´š |
|------|----------|----------|--------|
| **AWP å°æŠ—è¨“ç·´** | +0.005~0.015 | â­â­ ä¸­ | ğŸŸ¡ é«˜ |
| **ModernBERT-large** | +0.015~0.025 | â­â­ ä¸­ | ğŸŸ¡ é«˜ |
| **Combined Loss** | +0.008~0.015 | â­â­ ä¸­ | ğŸŸ¡ ä¸­ |

#### ğŸ“¦ éœ€è¦é©—è­‰ï¼ˆ1-2 å¤©ï¼‰

| æ–¹æ³• | é æœŸæå‡ | å¯¦ç¾é›£åº¦ | å„ªå…ˆç´š |
|------|----------|----------|--------|
| **æ¨¡å‹èåˆ (2-3 models)** | +0.01~0.02 | â­â­â­ é«˜ | ğŸŸ¢ ä¸­ |
| **çµæ§‹åŒ–è¼¸å…¥/é›™å¡”** | +0.003~0.008 | â­â­â­ é«˜ | ğŸŸ¢ ä½ |
| **Pseudo labeling** | +0.005~0.015 | â­â­â­ é«˜ | ğŸŸ¢ ä½ |

#### âŒ ä¸æ¨è–¦ï¼ˆä½æ€§åƒ¹æ¯”ï¼‰

| æ–¹æ³• | åŸå›  |
|------|------|
| æ•¸æ“šå¢å¼·ï¼ˆBack Translation ç­‰ï¼‰| å°æ•¸æ“šé›†æ•ˆæœæœ‰é™ï¼Œè¨ˆç®—æˆæœ¬é«˜ |
| æ›´å¤§æ¨¡å‹ï¼ˆ>2Bï¼‰| å°æ¸¬è©¦é›†å®¹æ˜“éæ“¬åˆï¼Œé‚Šéš›æ”¶ç›Šéæ¸› |
| è¤‡é›œ ensembleï¼ˆ>5 modelsï¼‰| å·¥ç¨‹é‡å¤§ï¼Œæ”¶ç›Šä¸å¦‚å„ªåŒ–å–®æ¨¡å‹ |

---

### 3.6 ğŸš€ æ¨è–¦å¯¦æ–½è·¯ç·šåœ–

```
ç¬¬ 1 é€±ï¼šå¿«é€Ÿæå‡
â”œâ”€â”€ Day 1: æ·»åŠ  EMA + Layer-wise LR Decay
â”œâ”€â”€ Day 2: å¯¦é©— Ranking Loss
â”œâ”€â”€ Day 3: å„ªåŒ– Distribution Matching
â””â”€â”€ Day 4: é©—è­‰æå‡ï¼Œæäº¤ Kaggle

ç¬¬ 2 é€±ï¼šé€²éšå„ªåŒ–
â”œâ”€â”€ Day 1-2: å¯¦ç¾ AWP å°æŠ—è¨“ç·´
â”œâ”€â”€ Day 3-4: æ¸¬è©¦ ModernBERT-large
â””â”€â”€ Day 5: æ¨¡å‹èåˆå¯¦é©—

ç¬¬ 3 é€±ï¼šç²¾ç´°èª¿å„ª
â”œâ”€â”€ è¶…åƒæ•¸æœç´¢ï¼ˆLR, dropout, warmupï¼‰
â”œâ”€â”€ å¾Œè™•ç†å„ªåŒ–ï¼ˆDM æ¬„ä½å¾®èª¿ï¼‰
â””â”€â”€ æœ€çµ‚æäº¤
```

**é æœŸç¸½æå‡**: 
- ä¿å®ˆä¼°è¨ˆï¼š+0.015~0.030
- æ¨‚è§€ä¼°è¨ˆï¼š+0.025~0.045

---

## é™„éŒ„ï¼šå¿«é€Ÿåƒè€ƒ

### è¨“ç·´å‘½ä»¤

```bash
# å¿«é€Ÿè¿­ä»£ï¼ˆå–® foldï¼‰
python src/train_and_inference_deberta.py --mode train --n_folds 1

# å®Œæ•´è¨“ç·´ï¼ˆ5 foldï¼‰
python src/train_and_inference_deberta.py --mode train --n_folds 5

# ä½¿ç”¨æ›´å¤§æ¨¡å‹
python src/train_and_inference_deberta.py --mode both --model_name microsoft/deberta-v3-large

# åƒ…æ¨ç†ï¼ˆä¸é‡æ–°è¨“ç·´ï¼‰
python src/train_and_inference_deberta.py --mode inference
```

### ä¸Šå‚³æ¨¡å‹åˆ° Kaggle

```bash
# ä¸Šå‚³ 5-fold æ¨¡å‹
./upload_models.sh
```

### é—œéµè¶…åƒæ•¸

```python
Config:
    model_name = "microsoft/deberta-v3-large"
    max_len = 512
    batch_size = 16
    lr = 1e-5          # backbone learning rate
    head_lr = 5e-5     # head learning rate
    epochs = 5
    n_folds = 5
```