{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "539a3e02",
   "metadata": {},
   "source": [
    "# Google Quest Q&A Labeling - Training & Inference\n",
    "\n",
    "This notebook implements a DeBERTa-based model for multi-label classification of Q&A pairs.\n",
    "\n",
    "## Overview\n",
    "- **Training**: Fine-tune DeBERTa-v3-base on 30 Q&A quality labels\n",
    "- **Inference**: Generate predictions on test data with post-processing\n",
    "- **Architecture**: Weighted layer pooling + multi-sample dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d445f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x74556e8d2710>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, get_linear_schedule_with_warmup\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enable anomaly detection for debugging (can be disabled in production)\n",
    "torch.autograd.set_detect_anomaly(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "518bbbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. Configuration\n",
    "# ==========================================\n",
    "class Config:\n",
    "    \"\"\"Training and model configuration\"\"\"\n",
    "    model_name = \"microsoft/deberta-v3-base\"\n",
    "    \n",
    "\n",
    "    max_len = 512\n",
    "    batch_size = 16         \n",
    "    accum_iter = 1          \n",
    "    \n",
    "    # Lower LR for stability with large batch size\n",
    "    lr = 1e-5               # Reduced from 2e-5 to prevent gradient explosion\n",
    "    head_lr = 5e-5          # Reduced from 1e-4\n",
    "    \n",
    "    # Training configuration\n",
    "    epochs = 3              # Reduced for fast iteration\n",
    "    n_folds = 1             # Single fold for fast testing (set to 5 for final training)\n",
    "    validation_split = 0.1  # 10% validation split when n_folds = 1\n",
    "    seed = 42\n",
    "    num_workers = 8         # Increased from 2 for better data loading\n",
    "    train_csv = \"../data/train.csv\"\n",
    "    test_csv = \"../data/test.csv\"\n",
    "    sample_submission_csv = \"../data/sample_submission.csv\"\n",
    "    output_dir = \"../models\"\n",
    "    \n",
    "    target_cols = [\n",
    "        'question_asker_intent_understanding', 'question_body_critical', 'question_conversational',\n",
    "        'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer',\n",
    "        'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent',\n",
    "        'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice',\n",
    "        'question_type_compare', 'question_type_consequence', 'question_type_definition',\n",
    "        'question_type_entity', 'question_type_instructions', 'question_type_procedure',\n",
    "        'question_type_reason_explanation', 'question_type_spelling', 'question_well_written',\n",
    "        'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n",
    "        'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure',\n",
    "        'answer_type_reason_explanation', 'answer_well_written'\n",
    "    ]\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  # Disable for reproducibility\n",
    "\n",
    "seed_everything(Config.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e8a4d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. Dataset Class\n",
    "# ==========================================\n",
    "class QuestDataset(Dataset):\n",
    "    \"\"\"Custom dataset for Q&A labeling task\"\"\"\n",
    "    \n",
    "    def __init__(self, df, tokenizer, max_len=512, mode=\"train\"):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.titles = df['question_title'].values\n",
    "        self.bodies = df['question_body'].values\n",
    "        self.answers = df['answer'].values\n",
    "        \n",
    "        if self.mode != \"test\":\n",
    "            self.targets = df[Config.target_cols].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title = str(self.titles[idx])\n",
    "        body = str(self.bodies[idx])\n",
    "        answer = str(self.answers[idx])\n",
    "        \n",
    "        # Combine question parts\n",
    "        q_text = title + \" \" + self.tokenizer.sep_token + \" \" + body\n",
    "        a_text = answer\n",
    "        \n",
    "        # Tokenize\n",
    "        q_tokens = self.tokenizer.tokenize(q_text)\n",
    "        a_tokens = self.tokenizer.tokenize(a_text)\n",
    "        \n",
    "        # Dynamic truncation with budget awareness\n",
    "        budget = self.max_len - 3  # [CLS], [SEP], [SEP]\n",
    "        if len(q_tokens) + len(a_tokens) > budget:\n",
    "            half = budget // 2\n",
    "            if len(a_tokens) > half and len(q_tokens) > half:\n",
    "                a_tokens = a_tokens[:half]\n",
    "                q_tokens = q_tokens[:budget - len(a_tokens)]\n",
    "            elif len(a_tokens) <= half:\n",
    "                q_tokens = q_tokens[:budget - len(a_tokens)]\n",
    "            else:\n",
    "                a_tokens = a_tokens[:budget - len(q_tokens)]\n",
    "                \n",
    "        # Build input IDs\n",
    "        ids = [self.tokenizer.cls_token_id] + \\\n",
    "              self.tokenizer.convert_tokens_to_ids(q_tokens) + \\\n",
    "              [self.tokenizer.sep_token_id] + \\\n",
    "              self.tokenizer.convert_tokens_to_ids(a_tokens) + \\\n",
    "              [self.tokenizer.sep_token_id]\n",
    "              \n",
    "        mask = [1] * len(ids)\n",
    "        padding_len = self.max_len - len(ids)\n",
    "        ids = ids + [self.tokenizer.pad_token_id] * padding_len\n",
    "        mask = mask + [0] * padding_len\n",
    "        \n",
    "        output = {\n",
    "            'input_ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(mask, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        if self.mode != \"test\":\n",
    "            # Add safety check and clipping with stricter bounds\n",
    "            labels = self.targets[idx].copy()\n",
    "            labels = np.clip(labels, 0.0, 1.0)  # Ensure labels are in [0, 1]\n",
    "            labels = np.nan_to_num(labels, nan=0.5, posinf=1.0, neginf=0.0)  # Replace NaN/Inf\n",
    "            # Additional check for any remaining invalid values\n",
    "            if np.any(np.isnan(labels)) or np.any(np.isinf(labels)):\n",
    "                labels = np.where(np.isnan(labels) | np.isinf(labels), 0.5, labels)\n",
    "\n",
    "            output['labels'] = torch.tensor(labels, dtype=torch.float32)        \n",
    "            \n",
    "        return output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69e0a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. Model Class\n",
    "# ==========================================\n",
    "class QuestDebertaModel(nn.Module):\n",
    "    \"\"\"DeBERTa model with weighted layer pooling and multi-sample dropout\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=Config.model_name, num_labels=30):\n",
    "        super().__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.config.output_hidden_states = True\n",
    "        self.model = AutoModel.from_pretrained(model_name, config=self.config)\n",
    "        \n",
    "        # Weighted layer pooling with stable initialization\n",
    "        n_weights = self.config.num_hidden_layers + 1\n",
    "        weights_init = torch.zeros(n_weights).float()\n",
    "        weights_init.data[:-1] = -3\n",
    "        weights_init.data[-1] = 0  # Give more weight to last layer initially\n",
    "        self.layer_weights = nn.Parameter(weights_init)\n",
    "        \n",
    "        # Multi-sample dropout\n",
    "        self.dropouts = nn.ModuleList([nn.Dropout(0.5) for _ in range(5)])\n",
    "        self.fc = nn.Linear(self.config.hidden_size, num_labels)\n",
    "        # Removed Sigmoid for BCEWithLogitsLoss stability\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.hidden_states \n",
    "        \n",
    "        # Stack [CLS] tokens\n",
    "        cls_outputs = torch.stack([layer[:, 0, :] for layer in hidden_states], dim=1)\n",
    "        \n",
    "        # Weighted sum\n",
    "        weights = torch.softmax(self.layer_weights, dim=0).view(1, -1, 1)\n",
    "        weighted_cls = (weights * cls_outputs).sum(dim=1)\n",
    "        \n",
    "        # Multi-sample dropout\n",
    "        logits_list = []\n",
    "        for dropout in self.dropouts:\n",
    "            logits_list.append(self.fc(dropout(weighted_cls)))\n",
    "        avg_logits = torch.mean(torch.stack(logits_list, dim=0), dim=0)\n",
    "        \n",
    "        return avg_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87272f2",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d9820cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. Utilities\n",
    "# ==========================================\n",
    "def compute_spearmanr(trues, preds):\n",
    "    \"\"\"Compute mean Spearman correlation across all labels\"\"\"\n",
    "    scores = []\n",
    "    for i in range(trues.shape[1]):\n",
    "        # Handle edge cases where all values are the same\n",
    "        if len(np.unique(trues[:, i])) == 1 or len(np.unique(preds[:, i])) == 1:\n",
    "            # If all predictions or all true values are the same, skip this column\n",
    "            continue\n",
    "        \n",
    "        corr = spearmanr(trues[:, i], preds[:, i]).correlation\n",
    "        \n",
    "        # Skip NaN correlations\n",
    "        if not np.isnan(corr):\n",
    "            scores.append(corr)\n",
    "    \n",
    "    # Return mean of valid scores, or 0 if no valid scores\n",
    "    return np.mean(scores) if len(scores) > 0 else 0.0\n",
    "\n",
    "def create_dataloaders(train_df, val_df, tokenizer):\n",
    "    \"\"\"Create train and validation dataloaders\"\"\"\n",
    "    train_dataset = QuestDataset(train_df, tokenizer, mode=\"train\")\n",
    "    val_dataset = QuestDataset(val_df, tokenizer, mode=\"train\")\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=Config.batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=Config.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=Config.batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=Config.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "    \"\"\"Differential learning rate for backbone and head\"\"\"\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "         'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "    ]\n",
    "    return optimizer_parameters\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, scheduler, loss_fn, device, accum_iter):\n",
    "    \"\"\"Train one epoch with gradient accumulation\"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Check for NaN loss\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"\\n⚠ Warning: NaN/Inf loss detected at step {step}, skipping batch\")\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "        \n",
    "        # Gradient accumulation\n",
    "        loss = loss / accum_iter\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % accum_iter == 0:\n",
    "            # Gradient clipping to prevent explosion\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        train_loss += loss.item() * accum_iter\n",
    "        progress_bar.set_postfix({'loss': train_loss / (step + 1)})\n",
    "    \n",
    "    return train_loss / len(train_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, val_loader, device):\n",
    "    \"\"\"Validation loop\"\"\"\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_trues = []\n",
    "    \n",
    "    for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, mask)\n",
    "        # Apply sigmoid for metrics since we removed it from model\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        \n",
    "        val_preds.append(outputs.cpu().numpy())\n",
    "        val_trues.append(labels.cpu().numpy())\n",
    "    \n",
    "    val_preds = np.concatenate(val_preds)\n",
    "    val_trues = np.concatenate(val_trues)\n",
    "    \n",
    "    # Add safety checks for predictions\n",
    "    val_preds = np.clip(val_preds, 0, 1)\n",
    "    val_preds = np.nan_to_num(val_preds, nan=0.5, posinf=1.0, neginf=0.0)\n",
    "    \n",
    "    score = compute_spearmanr(val_trues, val_preds)\n",
    "    return score, val_preds, val_trues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26e6201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. Training Pipeline (K-Fold with Multi-GPU)\n",
    "# ==========================================\n",
    "def train_loop():\n",
    "    \"\"\"Main training pipeline with GroupKFold and multi-GPU support\"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(Config.output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    train_df = pd.read_csv(Config.train_csv)\n",
    "    print(f\"Total samples: {len(train_df)}\\n\")\n",
    "    \n",
    "    # Check GPU availability\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print()\n",
    "    \n",
    "    # Choose between K-Fold and simple split based on n_folds\n",
    "    if Config.n_folds == 1:\n",
    "        # Fast single split for iteration testing\n",
    "        print(\"Using single train/val split for fast iteration\")\n",
    "        from sklearn.model_selection import GroupShuffleSplit\n",
    "        gss = GroupShuffleSplit(n_splits=1, test_size=Config.validation_split, random_state=Config.seed)\n",
    "        fold_splits = list(gss.split(train_df, train_df[Config.target_cols], groups=train_df['question_body']))\n",
    "    else:\n",
    "        # Full K-Fold for final training\n",
    "        print(f\"Using {Config.n_folds}-Fold Cross Validation\")\n",
    "        gkf = GroupKFold(n_splits=Config.n_folds)\n",
    "        fold_splits = list(gkf.split(train_df, train_df[Config.target_cols], groups=train_df['question_body']))\n",
    "    \n",
    "    # Store OOF predictions\n",
    "    oof_preds = np.zeros((len(train_df), len(Config.target_cols)))\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(fold_splits):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        if Config.n_folds == 1:\n",
    "            print(f\"Training (Single Split)\")\n",
    "        else:\n",
    "            print(f\"Fold {fold+1}/{Config.n_folds}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        train_data = train_df.iloc[train_idx].reset_index(drop=True)\n",
    "        val_data = train_df.iloc[val_idx].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Train samples: {len(train_data):,} | Val samples: {len(val_data):,}\")\n",
    "\n",
    "        # Initialize tokenizer and dataloaders\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.model_name)\n",
    "        train_loader, val_loader = create_dataloaders(train_data, val_data, tokenizer)\n",
    "\n",
    "        # Initialize model\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = QuestDebertaModel()\n",
    "        model.to(device)\n",
    "\n",
    "        # Differential Learning Rate\n",
    "        optimizer_parameters = get_optimizer_params(model, encoder_lr=Config.lr, decoder_lr=Config.head_lr)\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(optimizer_parameters, weight_decay=0.01)\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        # Scheduler\n",
    "        num_train_steps = int(len(train_loader) / Config.accum_iter * Config.epochs)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, \n",
    "            num_warmup_steps=int(0.1 * num_train_steps), \n",
    "            num_training_steps=num_train_steps\n",
    "        )\n",
    "\n",
    "        best_score = -1.0\n",
    "        best_model_path = os.path.join(Config.output_dir, f\"best_model_fold{fold+1}.pth\")\n",
    "        patience_counter = 0\n",
    "        patience = 3  # Early stopping patience (increased for stability)\n",
    "\n",
    "        for epoch in range(Config.epochs):\n",
    "            # Training\n",
    "            train_loss = train_epoch(model, train_loader, optimizer, scheduler, loss_fn, device, Config.accum_iter)\n",
    "            \n",
    "            # Validation\n",
    "            val_score, val_preds, _ = validate(model, val_loader, device)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1:2d}/{Config.epochs} | Loss: {train_loss:.4f} | Val Score: {val_score:.5f}\", end=\"\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_score > best_score:\n",
    "                best_score = val_score\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                print(f\" ✓ (New best)\")\n",
    "                print(f\"  Model saved to: {best_model_path}\")\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                print()\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "                \n",
    "        # Load best model for OOF\n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "        \n",
    "        _, val_preds, _ = validate(model, val_loader, device)\n",
    "        oof_preds[val_idx] = val_preds\n",
    "        \n",
    "        if Config.n_folds == 1:\n",
    "            print(f\"\\nBest validation score: {best_score:.5f}\")\n",
    "        else:\n",
    "            print(f\"\\nFold {fold+1} best score: {best_score:.5f}\")\n",
    "        \n",
    "        # Clean up\n",
    "        del model, optimizer, scheduler, train_loader, val_loader\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # Calculate overall CV score\n",
    "    overall_score = compute_spearmanr(train_df[Config.target_cols].values, oof_preds)\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Overall CV Spearman Score: {overall_score:.5f}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    # Save OOF predictions\n",
    "    np.save(os.path.join(Config.output_dir, \"oof_preds.npy\"), oof_preds)\n",
    "    \n",
    "    return oof_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b8e0798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Total samples: 6079\n",
      "\n",
      "Number of GPUs available: 2\n",
      "  GPU 0: NVIDIA RTX PRO 6000 Blackwell Server Edition\n",
      "  GPU 1: NVIDIA RTX PRO 6000 Blackwell Server Edition\n",
      "\n",
      "Using single train/val split for fast iteration\n",
      "\n",
      "==================================================\n",
      "Training (Single Split)\n",
      "==================================================\n",
      "Train samples: 5,453 | Val samples: 626\n",
      "Total samples: 6079\n",
      "\n",
      "Number of GPUs available: 2\n",
      "  GPU 0: NVIDIA RTX PRO 6000 Blackwell Server Edition\n",
      "  GPU 1: NVIDIA RTX PRO 6000 Blackwell Server Edition\n",
      "\n",
      "Using single train/val split for fast iteration\n",
      "\n",
      "==================================================\n",
      "Training (Single Split)\n",
      "==================================================\n",
      "Train samples: 5,453 | Val samples: 626\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1467ee77ce2b4a5fb2151145582d19f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/341 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7172c1dec9a34287b99d730a9e7ea792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/3 | Loss: 0.4663 | Val Score: 0.29881 ✓ (New best)\n",
      "  Model saved to: ../models/best_model_fold1.pth\n",
      " ✓ (New best)\n",
      "  Model saved to: ../models/best_model_fold1.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756593c10a6c4ad786e93dd150f6515a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/341 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2859db243844ef94913173fa1adf54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2/3 | Loss: 0.3896 | Val Score: 0.33622 ✓ (New best)\n",
      "  Model saved to: ../models/best_model_fold1.pth\n",
      " ✓ (New best)\n",
      "  Model saved to: ../models/best_model_fold1.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56fde317fff4b1db1d49d1ab8ac2acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/341 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f59b32cd2ef4ef3a921df54fb0f8a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3/3 | Loss: 0.3789 | Val Score: 0.34490 ✓ (New best)\n",
      "  Model saved to: ../models/best_model_fold1.pth\n",
      " ✓ (New best)\n",
      "  Model saved to: ../models/best_model_fold1.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fbe502db7ff44d8b110e1791a88d274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best validation score: 0.34490\n",
      "\n",
      "==================================================\n",
      "Overall CV Spearman Score: 0.00342\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "Overall CV Spearman Score: 0.00342\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "model = train_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7569c44e",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "Load trained model and generate predictions on test set with post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84de5568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 6. Post-processing Utilities\n",
    "# ==========================================\n",
    "def get_valid_bins(train_df, target_cols):\n",
    "    \"\"\"Extract valid bin values from training data\"\"\"\n",
    "    unique_bins = {}\n",
    "    for col in target_cols:\n",
    "        bins = np.sort(train_df[col].unique())\n",
    "        unique_bins[col] = bins\n",
    "    return unique_bins\n",
    "\n",
    "def snap_predictions(predictions, target_cols, unique_bins):\n",
    "    \"\"\"Round predictions to nearest valid values from training set\"\"\"\n",
    "    snapped = predictions.copy()\n",
    "    for i, col in enumerate(target_cols):\n",
    "        valid_vals = unique_bins[col]\n",
    "        \n",
    "        # Only snap sparse columns (< 100 unique values)\n",
    "        if len(valid_vals) > 100:\n",
    "            continue\n",
    "            \n",
    "        col_preds = snapped[:, i].reshape(-1, 1)\n",
    "        diffs = np.abs(col_preds - valid_vals.reshape(1, -1))\n",
    "        min_indices = np.argmin(diffs, axis=1)\n",
    "        snapped[:, i] = valid_vals[min_indices]\n",
    "    return snapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0baf314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 7. Inference Pipeline (Ensemble with Multi-GPU)\n",
    "# ==========================================\n",
    "@torch.no_grad()\n",
    "def generate_predictions(model, test_loader, device):\n",
    "    \"\"\"Generate predictions on test set\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    for batch in tqdm(test_loader, desc=\"Inference\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, mask)\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        all_preds.append(outputs.cpu().numpy())\n",
    "            \n",
    "    return np.concatenate(all_preds)\n",
    "\n",
    "def inference_pipeline():\n",
    "    \"\"\"Complete inference pipeline with 5-fold ensemble and multi-GPU support\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Available GPUs: {torch.cuda.device_count()}\\n\")\n",
    "    \n",
    "    # Load test data\n",
    "    print(\"Loading test data...\")\n",
    "    test_df = pd.read_csv(Config.test_csv)\n",
    "    print(f\"Test samples: {len(test_df):,}\\n\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Config.model_name)\n",
    "    \n",
    "    # Prepare test dataloader with larger batch size for inference\n",
    "    test_dataset = QuestDataset(test_df, tokenizer, mode=\"test\")\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=Config.batch_size * 4,  # Larger batch for inference\n",
    "        shuffle=False, \n",
    "        num_workers=Config.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Ensemble predictions\n",
    "    fold_preds = []\n",
    "    \n",
    "    for fold in range(1, Config.n_folds + 1):\n",
    "        model_path = os.path.join(Config.output_dir, f\"best_model_fold{fold}.pth\")\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"⚠ Model for fold {fold} not found, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Loading model fold {fold}...\")\n",
    "        model = QuestDebertaModel()\n",
    "        state_dict = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(device)\n",
    "        \n",
    "        preds = generate_predictions(model, test_loader, device)\n",
    "        fold_preds.append(preds)\n",
    "        \n",
    "        # Clean up\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    if not fold_preds:\n",
    "        raise ValueError(\"No models found for inference!\")\n",
    "        \n",
    "    # Average predictions\n",
    "    avg_preds = np.mean(fold_preds, axis=0)\n",
    "    print(f\"\\n✓ Generated predictions from {len(fold_preds)} fold models\")\n",
    "    \n",
    "    # Post-processing\n",
    "    print(\"\\nApplying post-processing...\")\n",
    "    if os.path.exists(Config.train_csv):\n",
    "        train_df = pd.read_csv(Config.train_csv)\n",
    "        bins_dict = get_valid_bins(train_df, Config.target_cols)\n",
    "        final_preds = snap_predictions(avg_preds, Config.target_cols, bins_dict)\n",
    "        print(\"✓ Predictions snapped to valid values\")\n",
    "    else:\n",
    "        final_preds = avg_preds\n",
    "        print(\"⚠ train.csv not found, skipping post-processing\")\n",
    "\n",
    "    # Create submission\n",
    "    print(\"\\nCreating submission file...\")\n",
    "    submission = pd.DataFrame(final_preds, columns=Config.target_cols)\n",
    "    submission['qa_id'] = test_df['qa_id']\n",
    "    submission = submission[['qa_id'] + Config.target_cols]\n",
    "    \n",
    "    output_path = os.path.join(Config.output_dir, \"submission.csv\")\n",
    "    submission.to_csv(output_path, index=False)\n",
    "    print(f\"✓ Submission saved to {output_path}\")\n",
    "    \n",
    "    return submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d81c5c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Available GPUs: 2\n",
      "\n",
      "Loading test data...\n",
      "Test samples: 476\n",
      "\n",
      "Loading model fold 1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f744945f889b44f2a7860295d5a5cdeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Generated predictions from 1 fold models\n",
      "\n",
      "Applying post-processing...\n",
      "✓ Predictions snapped to valid values\n",
      "\n",
      "Creating submission file...\n",
      "✓ Submission saved to ../models/submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Run inference\n",
    "submission = inference_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "google_quest_qa_labeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
