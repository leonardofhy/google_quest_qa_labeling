{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef6268dd",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614e7d30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T06:25:15.524827Z",
     "iopub.status.busy": "2025-11-27T06:25:15.523952Z",
     "iopub.status.idle": "2025-11-27T06:25:15.531508Z",
     "shell.execute_reply": "2025-11-27T06:25:15.529690Z",
     "shell.execute_reply.started": "2025-11-27T06:25:15.524801Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA device: Tesla P100-PCIE-16GB\n",
      "Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. è¨­ç½® Protobuf ä½¿ç”¨ Python å¯¦ç¾\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "# 2. é—œéµä¿®å¾©ï¼šå±è”½ TensorFlow\n",
    "# é€™æœƒé˜²æ­¢ transformers å°Žå…¥ tensorflowï¼Œå¾žè€Œé¿å… protobuf ç‰ˆæœ¬è¡çª\n",
    "sys.modules[\"tensorflow\"] = None \n",
    "\n",
    "import gc\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b20c58d8-2a44-46b0-b7fb-e37782ba99d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T06:25:15.533011Z",
     "iopub.status.busy": "2025-11-27T06:25:15.532796Z",
     "iopub.status.idle": "2025-11-27T06:25:15.558316Z",
     "shell.execute_reply": "2025-11-27T06:25:15.557662Z",
     "shell.execute_reply.started": "2025-11-27T06:25:15.532996Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/google-quest-challenge/sample_submission.csv\n",
      "/kaggle/input/google-quest-challenge/train.csv\n",
      "/kaggle/input/google-quest-challenge/test.csv\n",
      "/kaggle/input/deberta-v3-large-offline/spm.model\n",
      "/kaggle/input/deberta-v3-large-offline/config.json\n",
      "/kaggle/input/deberta-v3-large-offline/tokenizer.json\n",
      "/kaggle/input/deberta-v3-large-offline/tokenizer_config.json\n",
      "/kaggle/input/deberta-v3-large-offline/model.safetensors\n",
      "/kaggle/input/deberta-v3-large-offline/special_tokens_map.json\n",
      "/kaggle/input/deberta-v3-large-offline/added_tokens.json\n",
      "/kaggle/input/deberta-v3-large-quest-qa-5fold/training_history_fold5.json\n",
      "/kaggle/input/deberta-v3-large-quest-qa-5fold/training_history_fold1.json\n",
      "/kaggle/input/deberta-v3-large-quest-qa-5fold/training_summary.json\n",
      "/kaggle/input/deberta-v3-large-quest-qa-5fold/best_model_fold5.pth\n",
      "/kaggle/input/deberta-v3-large-quest-qa-5fold/training_history_fold3.json\n",
      "/kaggle/input/deberta-v3-large-quest-qa-5fold/best_model_fold2.pth\n",
      "/kaggle/input/deberta-v3-large-quest-qa-5fold/oof_preds.npy\n",
      "/kaggle/input/deberta-v3-large-quest-qa-5fold/submission.csv\n",
      "/kaggle/input/deberta-v3-large-quest-qa-5fold/training_history_fold2.json\n",
      "/kaggle/input/deberta-v3-large-quest-qa-5fold/best_model_fold1.pth\n",
      "/kaggle/input/deberta-v3-large-quest-qa-5fold/training_history_fold4.json\n",
      "/kaggle/input/deberta-v3-large-quest-qa-5fold/best_model_fold4.pth\n",
      "/kaggle/input/deberta-v3-large-quest-qa-5fold/best_model_fold3.pth\n"
     ]
    }
   ],
   "source": [
    "# éæ­· input ç›®éŒ„ï¼ŒæŸ¥çœ‹æª”æ¡ˆçµæ§‹\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2400497f",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82637f23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T06:25:15.560402Z",
     "iopub.status.busy": "2025-11-27T06:25:15.559954Z",
     "iopub.status.idle": "2025-11-27T06:25:15.568288Z",
     "shell.execute_reply": "2025-11-27T06:25:15.567693Z",
     "shell.execute_reply.started": "2025-11-27T06:25:15.560383Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration loaded successfully!\n",
      "\n",
      "ðŸ¤– Model: /kaggle/input/deberta-v3-large-offline\n",
      "ðŸ“ Max Length: 512\n",
      "ðŸ“¦ Batch Size: 16\n",
      "ðŸŽ² Random Seed: 42\n",
      "ðŸ”¢ Number of Folds: 5\n",
      "\n",
      "ðŸ“Š Number of target labels: 30\n",
      "\n",
      "ðŸ“‚ Model Directory: /kaggle/input/deberta-v3-large-quest-qa-5fold\n",
      "ðŸ“¤ Output Directory: /kaggle/working/\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Configuration\n",
    "# ==========================================\n",
    "class Config:\n",
    "    \"\"\"Inference configuration\"\"\"\n",
    "    # Model configuration\n",
    "    model_name = \"/kaggle/input/deberta-v3-large-offline\"\n",
    "    max_len = 512\n",
    "    batch_size = 16  # Can increase for inference (16-32 depending on GPU memory)\n",
    "    num_workers = 4\n",
    "    seed = 42\n",
    "    \n",
    "    # Paths\n",
    "    train_csv = \"/kaggle/input/google-quest-challenge/train.csv\"\n",
    "    test_csv = \"/kaggle/input/google-quest-challenge/test.csv\"\n",
    "    model_dir = \"/kaggle/input/deberta-v3-large-quest-qa-5fold\"\n",
    "    output_dir = \"/kaggle/working/\"\n",
    "    \n",
    "    # Model files (5 folds)\n",
    "    n_folds = 5\n",
    "    model_files = [f\"best_model_fold{i}.pth\" for i in range(1, n_folds + 1)]\n",
    "    \n",
    "    # Target columns (30 labels)\n",
    "    target_cols = [\n",
    "        'question_asker_intent_understanding', 'question_body_critical', 'question_conversational',\n",
    "        'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer',\n",
    "        'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent',\n",
    "        'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice',\n",
    "        'question_type_compare', 'question_type_consequence', 'question_type_definition',\n",
    "        'question_type_entity', 'question_type_instructions', 'question_type_procedure',\n",
    "        'question_type_reason_explanation', 'question_type_spelling', 'question_well_written',\n",
    "        'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n",
    "        'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure',\n",
    "        'answer_type_reason_explanation', 'answer_well_written'\n",
    "    ]\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(Config.seed)\n",
    "\n",
    "# Display configuration\n",
    "print(\"âœ… Configuration loaded successfully!\\n\")\n",
    "print(f\"ðŸ¤– Model: {Config.model_name}\")\n",
    "print(f\"ðŸ“ Max Length: {Config.max_len}\")\n",
    "print(f\"ðŸ“¦ Batch Size: {Config.batch_size}\")\n",
    "print(f\"ðŸŽ² Random Seed: {Config.seed}\")\n",
    "print(f\"ðŸ”¢ Number of Folds: {Config.n_folds}\")\n",
    "print(f\"\\nðŸ“Š Number of target labels: {len(Config.target_cols)}\")\n",
    "print(f\"\\nðŸ“‚ Model Directory: {Config.model_dir}\")\n",
    "print(f\"ðŸ“¤ Output Directory: {Config.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e2baeb",
   "metadata": {},
   "source": [
    "## 3. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5ae75b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T06:25:15.569974Z",
     "iopub.status.busy": "2025-11-27T06:25:15.569626Z",
     "iopub.status.idle": "2025-11-27T06:25:15.585203Z",
     "shell.execute_reply": "2025-11-27T06:25:15.584657Z",
     "shell.execute_reply.started": "2025-11-27T06:25:15.569954Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Dataset Class\n",
    "# ==========================================\n",
    "class QuestTestDataset(Dataset):\n",
    "    \"\"\"Custom dataset for Q&A labeling inference\"\"\"\n",
    "    \n",
    "    def __init__(self, df, tokenizer, max_len=512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.titles = df['question_title'].values\n",
    "        self.bodies = df['question_body'].values\n",
    "        self.answers = df['answer'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title = str(self.titles[idx])\n",
    "        body = str(self.bodies[idx])\n",
    "        answer = str(self.answers[idx])\n",
    "        \n",
    "        # Combine question parts: [CLS] title [SEP] body [SEP] answer [SEP]\n",
    "        q_text = title + \" \" + self.tokenizer.sep_token + \" \" + body\n",
    "        a_text = answer\n",
    "        \n",
    "        # Tokenize\n",
    "        q_tokens = self.tokenizer.tokenize(q_text)\n",
    "        a_tokens = self.tokenizer.tokenize(a_text)\n",
    "        \n",
    "        # Dynamic truncation with budget awareness\n",
    "        budget = self.max_len - 3  # Reserve for [CLS], [SEP], [SEP]\n",
    "        if len(q_tokens) + len(a_tokens) > budget:\n",
    "            half = budget // 2\n",
    "            if len(a_tokens) > half and len(q_tokens) > half:\n",
    "                # Both exceed half budget: truncate both\n",
    "                a_tokens = a_tokens[:half]\n",
    "                q_tokens = q_tokens[:budget - len(a_tokens)]\n",
    "            elif len(a_tokens) <= half:\n",
    "                # Answer fits in half: keep full answer, truncate question\n",
    "                q_tokens = q_tokens[:budget - len(a_tokens)]\n",
    "            else:\n",
    "                # Question fits in half: keep full question, truncate answer\n",
    "                a_tokens = a_tokens[:budget - len(q_tokens)]\n",
    "                \n",
    "        # Build input IDs: [CLS] + question_tokens + [SEP] + answer_tokens + [SEP]\n",
    "        ids = [self.tokenizer.cls_token_id] + \\\n",
    "              self.tokenizer.convert_tokens_to_ids(q_tokens) + \\\n",
    "              [self.tokenizer.sep_token_id] + \\\n",
    "              self.tokenizer.convert_tokens_to_ids(a_tokens) + \\\n",
    "              [self.tokenizer.sep_token_id]\n",
    "              \n",
    "        mask = [1] * len(ids)\n",
    "        padding_len = self.max_len - len(ids)\n",
    "        ids = ids + [self.tokenizer.pad_token_id] * padding_len\n",
    "        mask = mask + [0] * padding_len\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(mask, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"âœ… Dataset class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc45d7fe",
   "metadata": {},
   "source": [
    "## 4. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f71a9ccd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T06:25:15.585993Z",
     "iopub.status.busy": "2025-11-27T06:25:15.585826Z",
     "iopub.status.idle": "2025-11-27T06:25:15.609205Z",
     "shell.execute_reply": "2025-11-27T06:25:15.608597Z",
     "shell.execute_reply.started": "2025-11-27T06:25:15.585981Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Model Class\n",
    "# ==========================================\n",
    "class QuestDebertaModel(nn.Module):\n",
    "    \"\"\"DeBERTa model with weighted layer pooling and multi-sample dropout\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=Config.model_name, num_labels=30):\n",
    "        super().__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.config.output_hidden_states = True\n",
    "        self.model = AutoModel.from_pretrained(model_name, config=self.config)\n",
    "        \n",
    "        # Weighted layer pooling\n",
    "        # Combines all hidden layers with learnable weights\n",
    "        n_weights = self.config.num_hidden_layers + 1\n",
    "        weights_init = torch.zeros(n_weights).float()\n",
    "        weights_init.data[:-1] = -3  # Lower weight for earlier layers\n",
    "        weights_init.data[-1] = 0    # Higher weight for last layer\n",
    "        self.layer_weights = nn.Parameter(weights_init)\n",
    "        \n",
    "        # Multi-sample dropout (5 dropouts with 0.5 rate)\n",
    "        # Averages predictions from 5 different dropout masks\n",
    "        self.dropouts = nn.ModuleList([nn.Dropout(0.5) for _ in range(5)])\n",
    "        self.fc = nn.Linear(self.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get all hidden states\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.hidden_states \n",
    "        \n",
    "        # Extract [CLS] token from each layer: (batch, num_layers, hidden_size)\n",
    "        cls_outputs = torch.stack([layer[:, 0, :] for layer in hidden_states], dim=1)\n",
    "        \n",
    "        # Apply weighted sum across layers\n",
    "        weights = torch.softmax(self.layer_weights, dim=0).view(1, -1, 1)\n",
    "        weighted_cls = (weights * cls_outputs).sum(dim=1)  # (batch, hidden_size)\n",
    "        \n",
    "        # Multi-sample dropout: average predictions from 5 different dropout masks\n",
    "        logits_list = []\n",
    "        for dropout in self.dropouts:\n",
    "            logits_list.append(self.fc(dropout(weighted_cls)))\n",
    "        avg_logits = torch.mean(torch.stack(logits_list, dim=0), dim=0)\n",
    "        \n",
    "        # Return logits (sigmoid will be applied during inference)\n",
    "        return avg_logits\n",
    "\n",
    "print(\"âœ… Model class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efc5d96",
   "metadata": {},
   "source": [
    "## 5. Post-Processing Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45533976",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T06:25:15.645586Z",
     "iopub.status.busy": "2025-11-27T06:25:15.644990Z",
     "iopub.status.idle": "2025-11-27T06:25:15.654695Z",
     "shell.execute_reply": "2025-11-27T06:25:15.653815Z",
     "shell.execute_reply.started": "2025-11-27T06:25:15.645540Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Post-processing utilities defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Post-processing Utilities (Winning Solution)\n",
    "# ==========================================\n",
    "def postprocess_single_column(target, ref):\n",
    "    \"\"\"\n",
    "    Match the distribution of predicted column to training distribution.\n",
    "    \n",
    "    This technique from the winning solution adjusts predictions to follow\n",
    "    the same distribution as the training data. Since Spearman correlation\n",
    "    is rank-based, this can improve rankings.\n",
    "    \n",
    "    Args:\n",
    "        target: Predicted values for a single column (numpy array)\n",
    "        ref: Training values for the same column (numpy array)\n",
    "        \n",
    "    Returns:\n",
    "        Postprocessed predictions scaled to [0, 1]\n",
    "    \"\"\"\n",
    "    # Sort indices by predicted values\n",
    "    ids = np.argsort(target)\n",
    "    \n",
    "    # Get value counts from training data, sorted by value\n",
    "    counts = sorted(Counter(ref).items(), key=lambda s: s[0])\n",
    "    scores = np.zeros_like(target)\n",
    "    \n",
    "    last_pos = 0\n",
    "    v = 0\n",
    "    \n",
    "    # Assign rank values based on training distribution\n",
    "    for value, count in counts:\n",
    "        # Calculate position proportional to training distribution\n",
    "        next_pos = last_pos + int(round(count / len(ref) * len(target)))\n",
    "        if next_pos == last_pos:\n",
    "            next_pos += 1\n",
    "            \n",
    "        # Assign same score to samples in this range\n",
    "        cond = ids[last_pos:next_pos]\n",
    "        scores[cond] = v\n",
    "        last_pos = next_pos\n",
    "        v += 1\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    if scores.max() > 0:\n",
    "        return scores / scores.max()\n",
    "    return scores\n",
    "\n",
    "\n",
    "def postprocess_predictions(predictions, train_df, target_cols):\n",
    "    \"\"\"\n",
    "    Apply distribution matching and normalization to predictions.\n",
    "    \n",
    "    Args:\n",
    "        predictions: numpy array of shape (n_samples, n_targets)\n",
    "        train_df: Training dataframe with target columns\n",
    "        target_cols: List of target column names\n",
    "        \n",
    "    Returns:\n",
    "        Postprocessed predictions as numpy array\n",
    "    \"\"\"\n",
    "    postprocessed = predictions.copy()\n",
    "    \n",
    "    # Columns where distribution matching showed improvement\n",
    "    # (identified from winning solution's experiments)\n",
    "    distribution_matching_cols = {\n",
    "        'question_conversational',\n",
    "        'question_type_compare',\n",
    "        'question_type_definition',\n",
    "        'question_type_entity',\n",
    "        'question_has_commonly_accepted_answer',\n",
    "        'question_type_consequence',\n",
    "        'question_type_spelling',\n",
    "        'question_type_choice',\n",
    "        'question_not_really_a_question',\n",
    "        'question_multi_intent',\n",
    "        'question_type_procedure',\n",
    "        'question_type_instructions',\n",
    "        'answer_type_procedure',\n",
    "        'answer_type_instructions',\n",
    "        'question_expect_short_answer',\n",
    "        'answer_type_reason_explanation',\n",
    "    }\n",
    "    \n",
    "    for i, col in enumerate(target_cols):\n",
    "        if col in distribution_matching_cols:\n",
    "            # Apply distribution matching for specific columns\n",
    "            postprocessed[:, i] = postprocess_single_column(\n",
    "                postprocessed[:, i], \n",
    "                train_df[col].values\n",
    "            )\n",
    "        \n",
    "        # Scale all columns to [0, 1] interval\n",
    "        v = postprocessed[:, i]\n",
    "        v_min, v_max = v.min(), v.max()\n",
    "        if v_max > v_min:\n",
    "            postprocessed[:, i] = (v - v_min) / (v_max - v_min)\n",
    "        else:\n",
    "            postprocessed[:, i] = 0.5  # If all values are the same\n",
    "    \n",
    "    return postprocessed\n",
    "\n",
    "print(\"âœ… Post-processing utilities defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e6006e",
   "metadata": {},
   "source": [
    "## 6. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aabdf83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T06:25:15.656082Z",
     "iopub.status.busy": "2025-11-27T06:25:15.655886Z",
     "iopub.status.idle": "2025-11-27T06:25:15.722593Z",
     "shell.execute_reply": "2025-11-27T06:25:15.721935Z",
     "shell.execute_reply.started": "2025-11-27T06:25:15.656067Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "âœ… Test samples: 476\n",
      "\n",
      "Test data columns: ['qa_id', 'question_title', 'question_body', 'question_user_name', 'question_user_page', 'answer', 'answer_user_name', 'answer_user_page', 'url', 'category', 'host']\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_title</th>\n",
       "      <th>question_body</th>\n",
       "      <th>question_user_name</th>\n",
       "      <th>question_user_page</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_user_name</th>\n",
       "      <th>answer_user_page</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>host</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>Will leaving corpses lying around upset my pri...</td>\n",
       "      <td>I see questions/information online about how t...</td>\n",
       "      <td>Dylan</td>\n",
       "      <td>https://gaming.stackexchange.com/users/64471</td>\n",
       "      <td>There is no consequence for leaving corpses an...</td>\n",
       "      <td>Nelson868</td>\n",
       "      <td>https://gaming.stackexchange.com/users/97324</td>\n",
       "      <td>http://gaming.stackexchange.com/questions/1979...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>gaming.stackexchange.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>Url link to feature image in the portfolio</td>\n",
       "      <td>I am new to Wordpress. i have issue with Featu...</td>\n",
       "      <td>Anu</td>\n",
       "      <td>https://wordpress.stackexchange.com/users/72927</td>\n",
       "      <td>I think it is possible with custom fields.\\n\\n...</td>\n",
       "      <td>Irina</td>\n",
       "      <td>https://wordpress.stackexchange.com/users/27233</td>\n",
       "      <td>http://wordpress.stackexchange.com/questions/1...</td>\n",
       "      <td>TECHNOLOGY</td>\n",
       "      <td>wordpress.stackexchange.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>Is accuracy, recoil or bullet spread affected ...</td>\n",
       "      <td>To experiment I started a bot game, toggled in...</td>\n",
       "      <td>Konsta</td>\n",
       "      <td>https://gaming.stackexchange.com/users/37545</td>\n",
       "      <td>You do not have armour in the screenshots. Thi...</td>\n",
       "      <td>Damon Smithies</td>\n",
       "      <td>https://gaming.stackexchange.com/users/70641</td>\n",
       "      <td>http://gaming.stackexchange.com/questions/2154...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>gaming.stackexchange.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>Suddenly got an I/O error from my external HDD</td>\n",
       "      <td>I have used my Raspberry Pi as a torrent-serve...</td>\n",
       "      <td>robbannn</td>\n",
       "      <td>https://raspberrypi.stackexchange.com/users/17341</td>\n",
       "      <td>Your Western Digital hard drive is disappearin...</td>\n",
       "      <td>HeatfanJohn</td>\n",
       "      <td>https://raspberrypi.stackexchange.com/users/1311</td>\n",
       "      <td>http://raspberrypi.stackexchange.com/questions...</td>\n",
       "      <td>TECHNOLOGY</td>\n",
       "      <td>raspberrypi.stackexchange.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>Passenger Name - Flight Booking Passenger only...</td>\n",
       "      <td>I have bought Delhi-London return flights for ...</td>\n",
       "      <td>Amit</td>\n",
       "      <td>https://travel.stackexchange.com/users/29089</td>\n",
       "      <td>I called two persons who work for Saudia (tick...</td>\n",
       "      <td>Nean Der Thal</td>\n",
       "      <td>https://travel.stackexchange.com/users/10051</td>\n",
       "      <td>http://travel.stackexchange.com/questions/4704...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>travel.stackexchange.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id                                     question_title  \\\n",
       "0     39  Will leaving corpses lying around upset my pri...   \n",
       "1     46         Url link to feature image in the portfolio   \n",
       "2     70  Is accuracy, recoil or bullet spread affected ...   \n",
       "3    132     Suddenly got an I/O error from my external HDD   \n",
       "4    200  Passenger Name - Flight Booking Passenger only...   \n",
       "\n",
       "                                       question_body question_user_name  \\\n",
       "0  I see questions/information online about how t...              Dylan   \n",
       "1  I am new to Wordpress. i have issue with Featu...                Anu   \n",
       "2  To experiment I started a bot game, toggled in...             Konsta   \n",
       "3  I have used my Raspberry Pi as a torrent-serve...           robbannn   \n",
       "4  I have bought Delhi-London return flights for ...               Amit   \n",
       "\n",
       "                                  question_user_page  \\\n",
       "0       https://gaming.stackexchange.com/users/64471   \n",
       "1    https://wordpress.stackexchange.com/users/72927   \n",
       "2       https://gaming.stackexchange.com/users/37545   \n",
       "3  https://raspberrypi.stackexchange.com/users/17341   \n",
       "4       https://travel.stackexchange.com/users/29089   \n",
       "\n",
       "                                              answer answer_user_name  \\\n",
       "0  There is no consequence for leaving corpses an...        Nelson868   \n",
       "1  I think it is possible with custom fields.\\n\\n...            Irina   \n",
       "2  You do not have armour in the screenshots. Thi...   Damon Smithies   \n",
       "3  Your Western Digital hard drive is disappearin...      HeatfanJohn   \n",
       "4  I called two persons who work for Saudia (tick...    Nean Der Thal   \n",
       "\n",
       "                                   answer_user_page  \\\n",
       "0      https://gaming.stackexchange.com/users/97324   \n",
       "1   https://wordpress.stackexchange.com/users/27233   \n",
       "2      https://gaming.stackexchange.com/users/70641   \n",
       "3  https://raspberrypi.stackexchange.com/users/1311   \n",
       "4      https://travel.stackexchange.com/users/10051   \n",
       "\n",
       "                                                 url    category  \\\n",
       "0  http://gaming.stackexchange.com/questions/1979...     CULTURE   \n",
       "1  http://wordpress.stackexchange.com/questions/1...  TECHNOLOGY   \n",
       "2  http://gaming.stackexchange.com/questions/2154...     CULTURE   \n",
       "3  http://raspberrypi.stackexchange.com/questions...  TECHNOLOGY   \n",
       "4  http://travel.stackexchange.com/questions/4704...     CULTURE   \n",
       "\n",
       "                            host  \n",
       "0       gaming.stackexchange.com  \n",
       "1    wordpress.stackexchange.com  \n",
       "2       gaming.stackexchange.com  \n",
       "3  raspberrypi.stackexchange.com  \n",
       "4       travel.stackexchange.com  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Load Test Data\n",
    "# ==========================================\n",
    "print(\"Loading test data...\")\n",
    "test_df = pd.read_csv(Config.test_csv)\n",
    "print(f\"âœ… Test samples: {len(test_df):,}\")\n",
    "print(f\"\\nTest data columns: {list(test_df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70d7280",
   "metadata": {},
   "source": [
    "## 7. Prepare Test DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f02cb8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T06:25:15.724002Z",
     "iopub.status.busy": "2025-11-27T06:25:15.723610Z",
     "iopub.status.idle": "2025-11-27T06:25:16.113835Z",
     "shell.execute_reply": "2025-11-27T06:25:16.113126Z",
     "shell.execute_reply.started": "2025-11-27T06:25:15.723975Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "âœ… Tokenizer loaded: DebertaV2TokenizerFast\n",
      "\n",
      "Creating test dataset...\n",
      "âœ… Dataset size: 476\n",
      "\n",
      "Creating test dataloader...\n",
      "âœ… DataLoader created with batch_size=16\n",
      "   Total batches: 30\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Prepare Test DataLoader\n",
    "# ==========================================\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(Config.model_name)\n",
    "print(f\"âœ… Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "\n",
    "print(\"\\nCreating test dataset...\")\n",
    "test_dataset = QuestTestDataset(test_df, tokenizer, max_len=Config.max_len)\n",
    "print(f\"âœ… Dataset size: {len(test_dataset)}\")\n",
    "\n",
    "print(\"\\nCreating test dataloader...\")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=Config.batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=Config.num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "print(f\"âœ… DataLoader created with batch_size={Config.batch_size}\")\n",
    "print(f\"   Total batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2acba9b",
   "metadata": {},
   "source": [
    "## 8. Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a5a132e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T06:25:16.115180Z",
     "iopub.status.busy": "2025-11-27T06:25:16.114659Z",
     "iopub.status.idle": "2025-11-27T06:25:16.120745Z",
     "shell.execute_reply": "2025-11-27T06:25:16.119808Z",
     "shell.execute_reply.started": "2025-11-27T06:25:16.115154Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Inference function defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Inference Function\n",
    "# ==========================================\n",
    "@torch.no_grad()\n",
    "def generate_predictions(model, test_loader, device):\n",
    "    \"\"\"Generate predictions on test set\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    for batch in tqdm(test_loader, desc=\"Inference\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        # Get model outputs (logits)\n",
    "        outputs = model(input_ids, mask)\n",
    "        \n",
    "        # Apply sigmoid to get probabilities\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        \n",
    "        all_preds.append(outputs.cpu().numpy())\n",
    "            \n",
    "    return np.concatenate(all_preds)\n",
    "\n",
    "print(\"âœ… Inference function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be634c12",
   "metadata": {},
   "source": [
    "## 9. Run 5-Fold Ensemble Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdcd320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-27T06:25:16.122653Z",
     "iopub.status.busy": "2025-11-27T06:25:16.122358Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Memory: 17.06 GB\n",
      "\n",
      "============================================================\n",
      "Running 5-Fold Ensemble Inference\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Fold 1/5\n",
      "============================================================\n",
      "Loading model from: /kaggle/input/deberta-v3-large-quest-qa-5fold/best_model_fold1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 06:25:23.746439: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764224723.948137      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764224724.005097      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 5-Fold Ensemble Inference\n",
    "# ==========================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\\n\")\n",
    "\n",
    "# Store predictions from each fold\n",
    "fold_preds = []\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Running 5-Fold Ensemble Inference\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for fold in range(1, Config.n_folds + 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Fold {fold}/{Config.n_folds}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Model path\n",
    "    model_path = os.path.join(Config.model_dir, f\"best_model_fold{fold}.pth\")\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"âš ï¸  Model not found: {model_path}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = QuestDebertaModel(\n",
    "        model_name=Config.model_name,\n",
    "        num_labels=len(Config.target_cols)\n",
    "    )\n",
    "    \n",
    "    # Load weights\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"âœ… Model loaded successfully\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    print(f\"\\nGenerating predictions for fold {fold}...\")\n",
    "    preds = generate_predictions(model, test_loader, device)\n",
    "    fold_preds.append(preds)\n",
    "    \n",
    "    print(f\"âœ… Predictions shape: {preds.shape}\")\n",
    "    print(f\"   Min: {preds.min():.4f}, Max: {preds.max():.4f}, Mean: {preds.mean():.4f}\")\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    del model, state_dict\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   GPU Memory Used: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"All {len(fold_preds)} fold models processed successfully!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2c6f1c",
   "metadata": {},
   "source": [
    "## 10. Ensemble Predictions (Average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39ec357",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Ensemble Predictions\n",
    "# ==========================================\n",
    "print(\"\\nAveraging predictions across folds...\")\n",
    "avg_preds = np.mean(fold_preds, axis=0)\n",
    "\n",
    "print(f\"âœ… Ensemble predictions shape: {avg_preds.shape}\")\n",
    "print(f\"   Min: {avg_preds.min():.4f}\")\n",
    "print(f\"   Max: {avg_preds.max():.4f}\")\n",
    "print(f\"   Mean: {avg_preds.mean():.4f}\")\n",
    "print(f\"   Std: {avg_preds.std():.4f}\")\n",
    "\n",
    "# Display prediction statistics for each fold\n",
    "print(f\"\\nPer-fold statistics:\")\n",
    "for i, preds in enumerate(fold_preds, 1):\n",
    "    print(f\"  Fold {i}: Mean={preds.mean():.4f}, Std={preds.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f901a5",
   "metadata": {},
   "source": [
    "## 11. Apply Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b282d3f",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Apply Post-Processing\n",
    "# ==========================================\n",
    "print(\"\\nApplying post-processing with distribution matching...\")\n",
    "\n",
    "# Load training data for distribution matching\n",
    "if os.path.exists(Config.train_csv):\n",
    "    train_df = pd.read_csv(Config.train_csv)\n",
    "    print(f\"âœ… Training data loaded: {len(train_df)} samples\")\n",
    "    \n",
    "    # Apply post-processing\n",
    "    final_preds = postprocess_predictions(\n",
    "        avg_preds, \n",
    "        train_df, \n",
    "        Config.target_cols\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Distribution matching applied to 16 selected columns\")\n",
    "    print(f\"\\nPost-processed predictions:\")\n",
    "    print(f\"   Min: {final_preds.min():.4f}\")\n",
    "    print(f\"   Max: {final_preds.max():.4f}\")\n",
    "    print(f\"   Mean: {final_preds.mean():.4f}\")\n",
    "    print(f\"   Std: {final_preds.std():.4f}\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Training data not found at {Config.train_csv}\")\n",
    "    print(f\"   Skipping distribution matching post-processing\")\n",
    "    final_preds = avg_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74b3928",
   "metadata": {},
   "source": [
    "## 12. Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61add6a5",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Create Submission File\n",
    "# ==========================================\n",
    "print(\"\\nCreating submission file...\")\n",
    "\n",
    "# Create DataFrame with predictions\n",
    "submission = pd.DataFrame(final_preds, columns=Config.target_cols)\n",
    "\n",
    "# Add qa_id column\n",
    "submission['qa_id'] = test_df['qa_id'].values\n",
    "\n",
    "# Reorder columns: qa_id first, then all target columns\n",
    "submission = submission[['qa_id'] + Config.target_cols]\n",
    "\n",
    "print(f\"âœ… Submission DataFrame created\")\n",
    "print(f\"   Shape: {submission.shape}\")\n",
    "print(f\"   Columns: {len(submission.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae709d5",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "    # Create submission\n",
    "    print(\"\\nCreating submission file...\")\n",
    "    submission = pd.DataFrame(final_preds, columns=Config.target_cols)\n",
    "    submission['qa_id'] = test_df['qa_id'].values\n",
    "    submission = submission[['qa_id'] + Config.target_cols]\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_dir = \"/kaggle/working\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, \"submission.csv\")\n",
    "    \n",
    "    submission.to_csv(output_path, index=False)\n",
    "    print(f\"\\nâœ… Submission saved to: {output_path}\")\n",
    "    print(f\"   File size: {os.path.getsize(output_path) / 1024:.2f} KB\")\n",
    "    \n",
    "    # Verify submission format\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Submission Verification\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"âœ“ Number of rows: {len(submission)}\")\n",
    "    print(f\"âœ“ Number of columns: {len(submission.columns)}\")\n",
    "    print(f\"âœ“ qa_id column present: {'qa_id' in submission.columns}\")\n",
    "    print(f\"âœ“ All target columns present: {all(col in submission.columns for col in Config.target_cols)}\")\n",
    "    print(f\"âœ“ No missing values: {submission.isnull().sum().sum() == 0}\")\n",
    "    print(f\"âœ“ All values in [0, 1]: {(submission[Config.target_cols].min().min() >= 0) and (submission[Config.target_cols].max().max() <= 1)}\")\n",
    "    print(f\"\\nâœ… Submission file is ready for upload!\")\n",
    "    \n",
    "    return submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639a45ac",
   "metadata": {},
   "source": [
    "## 13. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bd4b87",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Summary Statistics\n",
    "# ==========================================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Inference Summary\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Model: microsoft/deberta-v3-large\")\n",
    "print(f\"Number of folds: {len(fold_preds)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"Target labels: {len(Config.target_cols)}\")\n",
    "print(f\"Post-processing: Distribution matching applied\")\n",
    "print(f\"\\nPrediction statistics per target:\")\n",
    "\n",
    "# Show statistics for each target\n",
    "stats_df = pd.DataFrame({\n",
    "    'Target': Config.target_cols,\n",
    "    'Min': final_preds.min(axis=0),\n",
    "    'Max': final_preds.max(axis=0),\n",
    "    'Mean': final_preds.mean(axis=0),\n",
    "    'Std': final_preds.std(axis=0)\n",
    "})\n",
    "\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb58e85d",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Display final statistics\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"âœ… INFERENCE COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"ðŸ“ Output Directory: {Config.output_dir}\")\n",
    "print(f\"ðŸ“Š Ready for submission!\")\n",
    "print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 828965,
     "sourceId": 7968,
     "sourceType": "competition"
    },
    {
     "datasetId": 8829497,
     "sourceId": 13859824,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8849306,
     "sourceId": 13890266,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
