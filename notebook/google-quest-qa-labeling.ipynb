{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T20:48:25.861039Z",
     "iopub.status.busy": "2025-11-24T20:48:25.860788Z",
     "iopub.status.idle": "2025-11-24T20:48:25.865000Z",
     "shell.execute_reply": "2025-11-24T20:48:25.864188Z",
     "shell.execute_reply.started": "2025-11-24T20:48:25.861022Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. Ë®≠ÁΩÆ Protobuf ‰ΩøÁî® Python ÂØ¶Áèæ\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "# 2. ÈóúÈçµ‰øÆÂæ©ÔºöÂ±èËîΩ TensorFlow\n",
    "# ÈÄôÊúÉÈò≤Ê≠¢ transformers Â∞éÂÖ• tensorflowÔºåÂæûËÄåÈÅøÂÖç protobuf ÁâàÊú¨Ë°ùÁ™Å\n",
    "sys.modules[\"tensorflow\"] = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T20:48:25.866428Z",
     "iopub.status.busy": "2025-11-24T20:48:25.866174Z",
     "iopub.status.idle": "2025-11-24T20:48:25.877694Z",
     "shell.execute_reply": "2025-11-24T20:48:25.876971Z",
     "shell.execute_reply.started": "2025-11-24T20:48:25.866403Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T20:48:25.881061Z",
     "iopub.status.busy": "2025-11-24T20:48:25.880776Z",
     "iopub.status.idle": "2025-11-24T20:48:25.892234Z",
     "shell.execute_reply": "2025-11-24T20:48:25.891478Z",
     "shell.execute_reply.started": "2025-11-24T20:48:25.881036Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/google-quest-challenge/sample_submission.csv\n",
      "/kaggle/input/google-quest-challenge/train.csv\n",
      "/kaggle/input/google-quest-challenge/test.csv\n",
      "/kaggle/input/quest-finetuned-weights/best-deberta-v3-base-1.pth\n",
      "/kaggle/input/quest-finetuned-weights/best_deberta-v3-base-2.pth\n",
      "/kaggle/input/deberta-v3-base-offline/spm.model\n",
      "/kaggle/input/deberta-v3-base-offline/config.json\n",
      "/kaggle/input/deberta-v3-base-offline/tokenizer.json\n",
      "/kaggle/input/deberta-v3-base-offline/tokenizer_config.json\n",
      "/kaggle/input/deberta-v3-base-offline/model.safetensors\n",
      "/kaggle/input/deberta-v3-base-offline/special_tokens_map.json\n",
      "/kaggle/input/deberta-v3-base-offline/added_tokens.json\n"
     ]
    }
   ],
   "source": [
    "# ÈÅçÊ≠∑ input ÁõÆÈåÑÔºåÊü•ÁúãÊ™îÊ°àÁµêÊßã\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T20:48:25.901484Z",
     "iopub.status.busy": "2025-11-24T20:48:25.901292Z",
     "iopub.status.idle": "2025-11-24T20:48:25.910076Z",
     "shell.execute_reply": "2025-11-24T20:48:25.909352Z",
     "shell.execute_reply.started": "2025-11-24T20:48:25.901470Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded successfully!\n",
      "\n",
      "ü§ñ Model: /kaggle/input/deberta-v3-base-offline\n",
      "üìè Max Length: 512\n",
      "üì¶ Batch Size: 16\n",
      "üé≤ Random Seed: 42\n",
      "\n",
      "üìä Number of target labels: 30\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. Configuration\n",
    "# ==========================================\n",
    "class Config:\n",
    "    \"\"\"Inference configuration\"\"\"\n",
    "    model_name = \"/kaggle/input/deberta-v3-base-offline\"\n",
    "    max_len = 512\n",
    "    batch_size = 16  # Can increase batch size for inference\n",
    "    num_workers = 2\n",
    "    seed = 42\n",
    "    \n",
    "    # Paths - Updated for Kaggle environment\n",
    "    train_csv = \"/kaggle/input/google-quest-challenge/train.csv\" \n",
    "    test_csv = \"/kaggle/input/google-quest-challenge/test.csv\"\n",
    "    sample_submission_csv = \"/kaggle/input/google-quest-challenge/sample_submission.csv\"\n",
    "    \n",
    "    # Path to folder containing trained model weights (5 fold models)\n",
    "    models_dir = \"/kaggle/input/quest-finetuned-weights\"\n",
    "    n_folds = 5  # Number of fold models to ensemble\n",
    "    \n",
    "    target_cols = [\n",
    "        'question_asker_intent_understanding', 'question_body_critical', 'question_conversational',\n",
    "        'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer',\n",
    "        'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent',\n",
    "        'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice',\n",
    "        'question_type_compare', 'question_type_consequence', 'question_type_definition',\n",
    "        'question_type_entity', 'question_type_instructions', 'question_type_procedure',\n",
    "        'question_type_reason_explanation', 'question_type_spelling', 'question_well_written',\n",
    "        'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n",
    "        'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure',\n",
    "        'answer_type_reason_explanation', 'answer_well_written'\n",
    "    ]\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(Config.seed)\n",
    "\n",
    "# Display configuration\n",
    "print(\"‚úÖ Configuration loaded successfully!\\n\")\n",
    "print(f\"ü§ñ Model: {Config.model_name}\")\n",
    "print(f\"üìè Max Length: {Config.max_len}\")\n",
    "print(f\"üì¶ Batch Size: {Config.batch_size}\")\n",
    "print(f\"üéØ Number of Folds: {Config.n_folds}\")\n",
    "print(f\"üé≤ Random Seed: {Config.seed}\")\n",
    "print(f\"\\nüìä Number of target labels: {len(Config.target_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T20:48:25.911842Z",
     "iopub.status.busy": "2025-11-24T20:48:25.911374Z",
     "iopub.status.idle": "2025-11-24T20:48:25.921544Z",
     "shell.execute_reply": "2025-11-24T20:48:25.920842Z",
     "shell.execute_reply.started": "2025-11-24T20:48:25.911826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. Dataset Class\n",
    "# ==========================================\n",
    "class QuestDataset(Dataset):\n",
    "    \"\"\"Custom dataset for Q&A labeling task\"\"\"\n",
    "    \n",
    "    def __init__(self, df, tokenizer, max_len=512, mode=\"test\"):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.titles = df['question_title'].values\n",
    "        self.bodies = df['question_body'].values\n",
    "        self.answers = df['answer'].values\n",
    "        \n",
    "        if self.mode != \"test\":\n",
    "            self.targets = df[Config.target_cols].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title = str(self.titles[idx])\n",
    "        body = str(self.bodies[idx])\n",
    "        answer = str(self.answers[idx])\n",
    "        \n",
    "        # Combine question parts\n",
    "        q_text = title + \" \" + self.tokenizer.sep_token + \" \" + body\n",
    "        a_text = answer\n",
    "        \n",
    "        # Tokenize\n",
    "        q_tokens = self.tokenizer.tokenize(q_text)\n",
    "        a_tokens = self.tokenizer.tokenize(a_text)\n",
    "        \n",
    "        # Dynamic truncation with budget awareness\n",
    "        budget = self.max_len - 3  # [CLS], [SEP], [SEP]\n",
    "        if len(q_tokens) + len(a_tokens) > budget:\n",
    "            half = budget // 2\n",
    "            if len(a_tokens) > half and len(q_tokens) > half:\n",
    "                a_tokens = a_tokens[:half]\n",
    "                q_tokens = q_tokens[:budget - len(a_tokens)]\n",
    "            elif len(a_tokens) <= half:\n",
    "                q_tokens = q_tokens[:budget - len(a_tokens)]\n",
    "            else:\n",
    "                a_tokens = a_tokens[:budget - len(q_tokens)]\n",
    "                \n",
    "        # Build input IDs\n",
    "        ids = [self.tokenizer.cls_token_id] + \\\n",
    "              self.tokenizer.convert_tokens_to_ids(q_tokens) + \\\n",
    "              [self.tokenizer.sep_token_id] + \\\n",
    "              self.tokenizer.convert_tokens_to_ids(a_tokens) + \\\n",
    "              [self.tokenizer.sep_token_id]\n",
    "              \n",
    "        mask = [1] * len(ids)\n",
    "        padding_len = self.max_len - len(ids)\n",
    "        ids = ids + [self.tokenizer.pad_token_id] * padding_len\n",
    "        mask = mask + [0] * padding_len\n",
    "        \n",
    "        output = {\n",
    "            'input_ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(mask, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        if self.mode != \"test\":\n",
    "            output['labels'] = torch.tensor(self.targets[idx], dtype=torch.float)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T20:48:26.017398Z",
     "iopub.status.busy": "2025-11-24T20:48:26.017222Z",
     "iopub.status.idle": "2025-11-24T20:48:26.024273Z",
     "shell.execute_reply": "2025-11-24T20:48:26.023447Z",
     "shell.execute_reply.started": "2025-11-24T20:48:26.017385Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. Model Class\n",
    "# ==========================================\n",
    "class QuestDebertaModel(nn.Module):\n",
    "    \"\"\"DeBERTa model with weighted layer pooling and multi-sample dropout\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=Config.model_name, num_labels=30):\n",
    "        super().__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.config.output_hidden_states = True\n",
    "        self.model = AutoModel.from_pretrained(model_name, config=self.config)\n",
    "        \n",
    "        # Weighted layer pooling\n",
    "        n_weights = self.config.num_hidden_layers + 1\n",
    "        weights_init = torch.zeros(n_weights).float()\n",
    "        weights_init.data[:-1] = -3\n",
    "        self.layer_weights = nn.Parameter(weights_init)\n",
    "        \n",
    "        # Multi-sample dropout\n",
    "        self.dropouts = nn.ModuleList([nn.Dropout(0.5) for _ in range(5)])\n",
    "        self.fc = nn.Linear(self.config.hidden_size, num_labels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.hidden_states \n",
    "        \n",
    "        # Stack [CLS] tokens\n",
    "        cls_outputs = torch.stack([layer[:, 0, :] for layer in hidden_states], dim=1)\n",
    "        \n",
    "        # Weighted sum\n",
    "        weights = torch.softmax(self.layer_weights, dim=0).view(1, -1, 1)\n",
    "        weighted_cls = (weights * cls_outputs).sum(dim=1)\n",
    "        \n",
    "        # Multi-sample dropout\n",
    "        logits_list = []\n",
    "        for dropout in self.dropouts:\n",
    "            logits_list.append(self.fc(dropout(weighted_cls)))\n",
    "        avg_logits = torch.mean(torch.stack(logits_list, dim=0), dim=0)\n",
    "        \n",
    "        return self.sigmoid(avg_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T20:48:26.025882Z",
     "iopub.status.busy": "2025-11-24T20:48:26.025586Z",
     "iopub.status.idle": "2025-11-24T20:48:26.037779Z",
     "shell.execute_reply": "2025-11-24T20:48:26.037068Z",
     "shell.execute_reply.started": "2025-11-24T20:48:26.025855Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. Post-processing Utilities (Winning Solution Approach)\n",
    "# ==========================================\n",
    "def postprocess_single_column(target, ref):\n",
    "    \"\"\"\n",
    "    Match the distribution of predicted column to training distribution.\n",
    "    \n",
    "    This technique from the winning solution adjusts predictions to follow\n",
    "    the same distribution as the training data. Since Spearman correlation\n",
    "    is rank-based, this can improve rankings by leveraging training set\n",
    "    distribution knowledge.\n",
    "    \n",
    "    Args:\n",
    "        target: Predicted values for a single column (numpy array)\n",
    "        ref: Training values for the same column (numpy array)\n",
    "        \n",
    "    Returns:\n",
    "        Postprocessed predictions scaled to [0, 1]\n",
    "    \"\"\"\n",
    "    # Sort indices by predicted values\n",
    "    ids = np.argsort(target)\n",
    "    \n",
    "    # Get value counts from training data, sorted by value\n",
    "    counts = sorted(Counter(ref).items(), key=lambda s: s[0])\n",
    "    scores = np.zeros_like(target)\n",
    "    \n",
    "    last_pos = 0\n",
    "    v = 0\n",
    "    \n",
    "    # Assign rank values based on training distribution\n",
    "    for value, count in counts:\n",
    "        # Calculate position in test set proportional to training distribution\n",
    "        next_pos = last_pos + int(round(count / len(ref) * len(target)))\n",
    "        if next_pos == last_pos:\n",
    "            next_pos += 1\n",
    "            \n",
    "        # Assign same score to samples in this range\n",
    "        cond = ids[last_pos:next_pos]\n",
    "        scores[cond] = v\n",
    "        last_pos = next_pos\n",
    "        v += 1\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    if scores.max() > 0:\n",
    "        return scores / scores.max()\n",
    "    return scores\n",
    "\n",
    "\n",
    "def postprocess_predictions(predictions, train_df, target_cols, use_distribution_matching=True):\n",
    "    \"\"\"\n",
    "    Apply distribution matching and normalization to predictions.\n",
    "    \n",
    "    Since Spearman correlation only cares about rankings, not actual values:\n",
    "    - Snapping to specific values is NOT helpful\n",
    "    - Distribution matching CAN help by adjusting rankings\n",
    "    \n",
    "    Args:\n",
    "        predictions: numpy array of shape (n_samples, n_targets)\n",
    "        train_df: Training dataframe with target columns\n",
    "        target_cols: List of target column names\n",
    "        use_distribution_matching: If True, apply distribution matching to selected columns\n",
    "        \n",
    "    Returns:\n",
    "        Postprocessed predictions as numpy array\n",
    "    \"\"\"\n",
    "    postprocessed = predictions.copy()\n",
    "    \n",
    "    # Columns where distribution matching showed substantial improvement\n",
    "    # Winner reported 0.027-0.030 boost from this technique\n",
    "    distribution_matching_cols = {\n",
    "        # Original columns from winner's solution\n",
    "        'question_conversational',\n",
    "        'question_type_compare',\n",
    "        'question_type_definition',\n",
    "        'question_type_entity',\n",
    "        'question_has_commonly_accepted_answer',\n",
    "        'question_type_consequence',\n",
    "        'question_type_spelling',\n",
    "        \n",
    "        # Additional challenging targets with sparse/imbalanced distributions\n",
    "        'question_type_choice',\n",
    "        'question_not_really_a_question',\n",
    "        'question_multi_intent',\n",
    "        'question_type_procedure',\n",
    "        'question_type_instructions',\n",
    "        'answer_type_procedure',\n",
    "        'answer_type_instructions',\n",
    "        'question_expect_short_answer',\n",
    "        'answer_type_reason_explanation',\n",
    "    }\n",
    "    \n",
    "    for i, col in enumerate(target_cols):\n",
    "        if use_distribution_matching and col in distribution_matching_cols:\n",
    "            # Apply distribution matching for specific columns\n",
    "            scores = postprocess_single_column(\n",
    "                postprocessed[:, i], \n",
    "                train_df[col].values\n",
    "            )\n",
    "            postprocessed[:, i] = scores\n",
    "        \n",
    "        # Scale all columns to [0, 1] interval\n",
    "        v = postprocessed[:, i]\n",
    "        v_min, v_max = v.min(), v.max()\n",
    "        if v_max > v_min:\n",
    "            postprocessed[:, i] = (v - v_min) / (v_max - v_min)\n",
    "        else:\n",
    "            postprocessed[:, i] = 0.5  # If all values are the same\n",
    "    \n",
    "    return postprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T20:48:26.040947Z",
     "iopub.status.busy": "2025-11-24T20:48:26.040737Z",
     "iopub.status.idle": "2025-11-24T20:48:36.180655Z",
     "shell.execute_reply": "2025-11-24T20:48:36.179688Z",
     "shell.execute_reply.started": "2025-11-24T20:48:26.040933Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. Inference Pipeline (5-Fold Ensemble)\n",
    "# ==========================================\n",
    "@torch.no_grad()\n",
    "def generate_predictions(model, test_loader, device):\n",
    "    \"\"\"Generate predictions on test set\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    for batch in tqdm(test_loader, desc=\"Inference\", leave=False):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, mask)\n",
    "        all_preds.append(outputs.cpu().numpy())\n",
    "            \n",
    "    return np.concatenate(all_preds)\n",
    "\n",
    "\n",
    "def inference_pipeline(use_postprocessing=True):\n",
    "    \"\"\"\n",
    "    Complete inference pipeline with 5-fold ensemble and post-processing.\n",
    "    \n",
    "    Args:\n",
    "        use_postprocessing: If True, apply distribution matching post-processing\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   Available GPUs: {torch.cuda.device_count()}\\n\")\n",
    "    \n",
    "    # Load test data\n",
    "    print(\"üìÇ Loading test data...\")\n",
    "    if not os.path.exists(Config.test_csv):\n",
    "        print(f\"‚ùå Error: Test file not found at {Config.test_csv}\")\n",
    "        return None\n",
    "        \n",
    "    test_df = pd.read_csv(Config.test_csv)\n",
    "    print(f\"   Test samples: {len(test_df):,}\\n\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(\"üî§ Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Config.model_name)\n",
    "    print(\"   ‚úì Tokenizer ready\\n\")\n",
    "    \n",
    "    # Prepare test dataloader with larger batch size for inference\n",
    "    test_dataset = QuestDataset(test_df, tokenizer, mode=\"test\")\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=Config.batch_size * 2,  # Larger batch for inference\n",
    "        shuffle=False, \n",
    "        num_workers=Config.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # ========================================\n",
    "    # Ensemble predictions from all fold models\n",
    "    # ========================================\n",
    "    print(f\"üéØ Loading and ensembling {Config.n_folds} fold models...\")\n",
    "    fold_preds = []\n",
    "    \n",
    "    for fold in range(1, Config.n_folds + 1):\n",
    "        model_path = os.path.join(Config.models_dir, f\"best_model_fold{fold}.pth\")\n",
    "        \n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"   ‚ö†Ô∏è  Warning: Model fold {fold} not found at {model_path}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"   üì¶ Loading fold {fold} model...\")\n",
    "        model = QuestDebertaModel()\n",
    "        state_dict = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Generate predictions for this fold\n",
    "        preds = generate_predictions(model, test_loader, device)\n",
    "        fold_preds.append(preds)\n",
    "        print(f\"      ‚úì Fold {fold} predictions generated\")\n",
    "        \n",
    "        # Clean up GPU memory\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    if not fold_preds:\n",
    "        raise ValueError(\"‚ùå No models found for inference!\")\n",
    "    \n",
    "    # Average predictions across all folds\n",
    "    avg_preds = np.mean(fold_preds, axis=0)\n",
    "    print(f\"\\n‚úÖ Ensemble complete: Averaged predictions from {len(fold_preds)} fold model(s)\")\n",
    "    print(f\"   Final predictions shape: {avg_preds.shape}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # Post-processing with distribution matching\n",
    "    # ========================================\n",
    "    if use_postprocessing and os.path.exists(Config.train_csv):\n",
    "        print(\"\\nüîß Applying distribution matching post-processing...\")\n",
    "        train_df = pd.read_csv(Config.train_csv)\n",
    "        final_preds = postprocess_predictions(\n",
    "            avg_preds, \n",
    "            train_df, \n",
    "            Config.target_cols,\n",
    "            use_distribution_matching=True\n",
    "        )\n",
    "        print(\"   ‚úì Distribution matching applied to selected columns\")\n",
    "    else:\n",
    "        final_preds = avg_preds\n",
    "        if not os.path.exists(Config.train_csv):\n",
    "            print(\"\\n‚ö†Ô∏è  train.csv not found, skipping post-processing\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  Post-processing disabled\")\n",
    "\n",
    "    # ========================================\n",
    "    # Create submission file\n",
    "    # ========================================\n",
    "    print(\"\\nüìù Creating submission file...\")\n",
    "    submission = pd.DataFrame(final_preds, columns=Config.target_cols)\n",
    "    submission['qa_id'] = test_df['qa_id']\n",
    "    submission = submission[['qa_id'] + Config.target_cols]\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"   ‚úì Submission saved to submission.csv\")\n",
    "    \n",
    "    # Display sample predictions\n",
    "    print(\"\\nüìä Sample predictions (first 3 rows):\")\n",
    "    print(submission.head(3))\n",
    "            \n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T20:48:36.182060Z",
     "iopub.status.busy": "2025-11-24T20:48:36.181830Z",
     "iopub.status.idle": "2025-11-24T20:49:06.666588Z",
     "shell.execute_reply": "2025-11-24T20:49:06.665749Z",
     "shell.execute_reply.started": "2025-11-24T20:48:36.182043Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Loading test data...\n",
      "Test samples: 476\n",
      "\n",
      "Loading model and tokenizer...\n",
      "‚úì Loaded weights from /kaggle/input/quest-finetuned-weights/best_deberta-v3-base-2.pth\n",
      "‚úì Model and tokenizer ready\n",
      "\n",
      "Generating predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8051c890654fb88cf26917635145ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying post-processing...\n",
      "‚úì Predictions snapped to valid values\n",
      "‚úì Added epsilon noise to prevent constant columns\n",
      "\n",
      "Creating submission file...\n",
      "‚úì Submission saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 6. Run Inference (5-Fold Ensemble)\n",
    "# ==========================================\n",
    "print(\"=\"*60)\n",
    "print(\"Google Quest Q&A Labeling - Inference with 5-Fold Ensemble\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  ‚Ä¢ Model: {Config.model_name}\")\n",
    "print(f\"  ‚Ä¢ Ensemble: {Config.n_folds} fold models\")\n",
    "print(f\"  ‚Ä¢ Post-processing: Distribution Matching (Winning Solution)\")\n",
    "print(f\"  ‚Ä¢ Batch Size: {Config.batch_size}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "submission = inference_pipeline(use_postprocessing=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Inference pipeline completed successfully!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 828965,
     "sourceId": 7968,
     "sourceType": "competition"
    },
    {
     "datasetId": 8825600,
     "sourceId": 13854451,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8826659,
     "sourceId": 13855873,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
