{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c052f928",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T18:10:03.619133Z",
     "iopub.status.busy": "2025-11-24T18:10:03.618861Z",
     "iopub.status.idle": "2025-11-24T18:10:03.623358Z",
     "shell.execute_reply": "2025-11-24T18:10:03.622716Z",
     "shell.execute_reply.started": "2025-11-24T18:10:03.619114Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. è¨­ç½® Protobuf ä½¿ç”¨ Python å¯¦ç¾\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "# 2. é—œéµä¿®å¾©ï¼šå±è”½ TensorFlow\n",
    "# é€™æœƒé˜²æ­¢ transformers å°Žå…¥ tensorflowï¼Œå¾žè€Œé¿å… protobuf ç‰ˆæœ¬è¡çª\n",
    "sys.modules[\"tensorflow\"] = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd5cd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0425eff-df0f-4b45-84bc-8ab609f4a0a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T18:10:03.629079Z",
     "iopub.status.busy": "2025-11-24T18:10:03.628845Z",
     "iopub.status.idle": "2025-11-24T18:10:03.640652Z",
     "shell.execute_reply": "2025-11-24T18:10:03.639930Z",
     "shell.execute_reply.started": "2025-11-24T18:10:03.629053Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/deberta-v3-base-offline/spm.model\n",
      "/kaggle/input/deberta-v3-base-offline/config.json\n",
      "/kaggle/input/deberta-v3-base-offline/tokenizer.json\n",
      "/kaggle/input/deberta-v3-base-offline/tokenizer_config.json\n",
      "/kaggle/input/deberta-v3-base-offline/model.safetensors\n",
      "/kaggle/input/deberta-v3-base-offline/special_tokens_map.json\n",
      "/kaggle/input/deberta-v3-base-offline/added_tokens.json\n",
      "/kaggle/input/google-quest-challenge/sample_submission.csv\n",
      "/kaggle/input/google-quest-challenge/train.csv\n",
      "/kaggle/input/google-quest-challenge/test.csv\n",
      "/kaggle/input/quest-finetuned-weights/best-deberta-v3-base-ft.pth\n"
     ]
    }
   ],
   "source": [
    "# éæ­· input ç›®éŒ„ï¼ŒæŸ¥çœ‹æª”æ¡ˆçµæ§‹\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87884ba9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T18:10:03.641843Z",
     "iopub.status.busy": "2025-11-24T18:10:03.641648Z",
     "iopub.status.idle": "2025-11-24T18:10:03.650818Z",
     "shell.execute_reply": "2025-11-24T18:10:03.650156Z",
     "shell.execute_reply.started": "2025-11-24T18:10:03.641829Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration loaded successfully!\n",
      "\n",
      "ðŸ¤– Model: /kaggle/input/deberta-v3-base-offline\n",
      "ðŸ“ Max Length: 512\n",
      "ðŸ“¦ Batch Size: 16\n",
      "ðŸŽ² Random Seed: 42\n",
      "\n",
      "ðŸ“Š Number of target labels: 30\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. Configuration\n",
    "# ==========================================\n",
    "class Config:\n",
    "    \"\"\"Inference configuration\"\"\"\n",
    "    model_name = \"/kaggle/input/deberta-v3-base-offline\"\n",
    "    max_len = 512\n",
    "    batch_size = 16  # Can increase batch size for inference\n",
    "    num_workers = 2\n",
    "    seed = 42\n",
    "    \n",
    "    # Paths - Updated for Kaggle environment\n",
    "    train_csv = \"/kaggle/input/google-quest-challenge/train.csv\" \n",
    "    test_csv = \"/kaggle/input/google-quest-challenge/test.csv\"\n",
    "    sample_submission_csv = \"/kaggle/input/google-quest-challenge/sample_submission.csv\"\n",
    "    \n",
    "    # Path to the trained model weights\n",
    "    model_path = \"/kaggle/input/quest-finetuned-weights/best-deberta-v3-base-ft.pth\"\n",
    "    \n",
    "    target_cols = [\n",
    "        'question_asker_intent_understanding', 'question_body_critical', 'question_conversational',\n",
    "        'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer',\n",
    "        'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent',\n",
    "        'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice',\n",
    "        'question_type_compare', 'question_type_consequence', 'question_type_definition',\n",
    "        'question_type_entity', 'question_type_instructions', 'question_type_procedure',\n",
    "        'question_type_reason_explanation', 'question_type_spelling', 'question_well_written',\n",
    "        'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n",
    "        'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure',\n",
    "        'answer_type_reason_explanation', 'answer_well_written'\n",
    "    ]\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(Config.seed)\n",
    "\n",
    "# Display configuration\n",
    "print(\"âœ… Configuration loaded successfully!\\n\")\n",
    "print(f\"ðŸ¤– Model: {Config.model_name}\")\n",
    "print(f\"ðŸ“ Max Length: {Config.max_len}\")\n",
    "print(f\"ðŸ“¦ Batch Size: {Config.batch_size}\")\n",
    "print(f\"ðŸŽ² Random Seed: {Config.seed}\")\n",
    "print(f\"\\nðŸ“Š Number of target labels: {len(Config.target_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed2f8fa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T18:10:03.652027Z",
     "iopub.status.busy": "2025-11-24T18:10:03.651776Z",
     "iopub.status.idle": "2025-11-24T18:10:13.806269Z",
     "shell.execute_reply": "2025-11-24T18:10:13.805460Z",
     "shell.execute_reply.started": "2025-11-24T18:10:03.652012Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. Dataset Class\n",
    "# ==========================================\n",
    "class QuestDataset(Dataset):\n",
    "    \"\"\"Custom dataset for Q&A labeling task\"\"\"\n",
    "    \n",
    "    def __init__(self, df, tokenizer, max_len=512, mode=\"test\"):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.titles = df['question_title'].values\n",
    "        self.bodies = df['question_body'].values\n",
    "        self.answers = df['answer'].values\n",
    "        \n",
    "        if self.mode != \"test\":\n",
    "            self.targets = df[Config.target_cols].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        title = str(self.titles[idx])\n",
    "        body = str(self.bodies[idx])\n",
    "        answer = str(self.answers[idx])\n",
    "        \n",
    "        # Combine question parts\n",
    "        q_text = title + \" \" + self.tokenizer.sep_token + \" \" + body\n",
    "        a_text = answer\n",
    "        \n",
    "        # Tokenize\n",
    "        q_tokens = self.tokenizer.tokenize(q_text)\n",
    "        a_tokens = self.tokenizer.tokenize(a_text)\n",
    "        \n",
    "        # Dynamic truncation with budget awareness\n",
    "        budget = self.max_len - 3  # [CLS], [SEP], [SEP]\n",
    "        if len(q_tokens) + len(a_tokens) > budget:\n",
    "            half = budget // 2\n",
    "            if len(a_tokens) > half and len(q_tokens) > half:\n",
    "                a_tokens = a_tokens[:half]\n",
    "                q_tokens = q_tokens[:budget - len(a_tokens)]\n",
    "            elif len(a_tokens) <= half:\n",
    "                q_tokens = q_tokens[:budget - len(a_tokens)]\n",
    "            else:\n",
    "                a_tokens = a_tokens[:budget - len(q_tokens)]\n",
    "                \n",
    "        # Build input IDs\n",
    "        ids = [self.tokenizer.cls_token_id] + \\\n",
    "              self.tokenizer.convert_tokens_to_ids(q_tokens) + \\\n",
    "              [self.tokenizer.sep_token_id] + \\\n",
    "              self.tokenizer.convert_tokens_to_ids(a_tokens) + \\\n",
    "              [self.tokenizer.sep_token_id]\n",
    "              \n",
    "        mask = [1] * len(ids)\n",
    "        padding_len = self.max_len - len(ids)\n",
    "        ids = ids + [self.tokenizer.pad_token_id] * padding_len\n",
    "        mask = mask + [0] * padding_len\n",
    "        \n",
    "        output = {\n",
    "            'input_ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(mask, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        if self.mode != \"test\":\n",
    "            output['labels'] = torch.tensor(self.targets[idx], dtype=torch.float)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "beaa9b14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T18:10:13.807987Z",
     "iopub.status.busy": "2025-11-24T18:10:13.807798Z",
     "iopub.status.idle": "2025-11-24T18:10:13.825108Z",
     "shell.execute_reply": "2025-11-24T18:10:13.824365Z",
     "shell.execute_reply.started": "2025-11-24T18:10:13.807972Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. Model Class\n",
    "# ==========================================\n",
    "class QuestDebertaModel(nn.Module):\n",
    "    \"\"\"DeBERTa model with weighted layer pooling and multi-sample dropout\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=Config.model_name, num_labels=30):\n",
    "        super().__init__()\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.config.output_hidden_states = True\n",
    "        self.model = AutoModel.from_pretrained(model_name, config=self.config)\n",
    "        \n",
    "        # Weighted layer pooling\n",
    "        n_weights = self.config.num_hidden_layers + 1\n",
    "        weights_init = torch.zeros(n_weights).float()\n",
    "        weights_init.data[:-1] = -3\n",
    "        self.layer_weights = nn.Parameter(weights_init)\n",
    "        \n",
    "        # Multi-sample dropout\n",
    "        self.dropouts = nn.ModuleList([nn.Dropout(0.5) for _ in range(5)])\n",
    "        self.fc = nn.Linear(self.config.hidden_size, num_labels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.hidden_states \n",
    "        \n",
    "        # Stack [CLS] tokens\n",
    "        cls_outputs = torch.stack([layer[:, 0, :] for layer in hidden_states], dim=1)\n",
    "        \n",
    "        # Weighted sum\n",
    "        weights = torch.softmax(self.layer_weights, dim=0).view(1, -1, 1)\n",
    "        weighted_cls = (weights * cls_outputs).sum(dim=1)\n",
    "        \n",
    "        # Multi-sample dropout\n",
    "        logits_list = []\n",
    "        for dropout in self.dropouts:\n",
    "            logits_list.append(self.fc(dropout(weighted_cls)))\n",
    "        avg_logits = torch.mean(torch.stack(logits_list, dim=0), dim=0)\n",
    "        \n",
    "        return self.sigmoid(avg_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c877d624",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T18:10:13.826158Z",
     "iopub.status.busy": "2025-11-24T18:10:13.825920Z",
     "iopub.status.idle": "2025-11-24T18:10:13.836177Z",
     "shell.execute_reply": "2025-11-24T18:10:13.835354Z",
     "shell.execute_reply.started": "2025-11-24T18:10:13.826138Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. Post-processing Utilities\n",
    "# ==========================================\n",
    "def get_valid_bins(train_df, target_cols):\n",
    "    \"\"\"Extract valid bin values from training data\"\"\"\n",
    "    unique_bins = {}\n",
    "    for col in target_cols:\n",
    "        bins = np.sort(train_df[col].unique())\n",
    "        unique_bins[col] = bins\n",
    "    return unique_bins\n",
    "\n",
    "def snap_predictions(predictions, target_cols, unique_bins):\n",
    "    \"\"\"Round predictions to nearest valid values from training set\"\"\"\n",
    "    snapped = predictions.copy()\n",
    "    for i, col in enumerate(target_cols):\n",
    "        valid_vals = unique_bins[col]\n",
    "        \n",
    "        # Only snap sparse columns (< 100 unique values)\n",
    "        if len(valid_vals) > 100:\n",
    "            continue\n",
    "            \n",
    "        col_preds = snapped[:, i].reshape(-1, 1)\n",
    "        diffs = np.abs(col_preds - valid_vals.reshape(1, -1))\n",
    "        min_indices = np.argmin(diffs, axis=1)\n",
    "        snapped[:, i] = valid_vals[min_indices]\n",
    "    return snapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf5ba49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T18:10:13.837278Z",
     "iopub.status.busy": "2025-11-24T18:10:13.836997Z",
     "iopub.status.idle": "2025-11-24T18:10:13.856147Z",
     "shell.execute_reply": "2025-11-24T18:10:13.855346Z",
     "shell.execute_reply.started": "2025-11-24T18:10:13.837257Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. Inference Pipeline\n",
    "# ==========================================\n",
    "@torch.no_grad()\n",
    "def generate_predictions(model, test_loader, device):\n",
    "    \"\"\"Generate predictions on test set\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    \n",
    "    print(\"Generating predictions...\")\n",
    "    for batch in tqdm(test_loader, desc=\"Inference\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, mask)\n",
    "        all_preds.append(outputs.cpu().numpy())\n",
    "            \n",
    "    return np.concatenate(all_preds)\n",
    "\n",
    "def inference_pipeline():\n",
    "    \"\"\"Complete inference pipeline\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\\n\")\n",
    "    \n",
    "    # Load test data\n",
    "    print(\"Loading test data...\")\n",
    "    if not os.path.exists(Config.test_csv):\n",
    "        print(f\"Error: Test file not found at {Config.test_csv}\")\n",
    "        return None\n",
    "        \n",
    "    test_df = pd.read_csv(Config.test_csv)\n",
    "    print(f\"Test samples: {len(test_df)}\\n\")\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(Config.model_name)\n",
    "        model = QuestDebertaModel()\n",
    "        \n",
    "        if os.path.exists(Config.model_path):\n",
    "            state_dict = torch.load(Config.model_path, map_location=device)\n",
    "            model.load_state_dict(state_dict)\n",
    "            print(f\"âœ“ Loaded weights from {Config.model_path}\")\n",
    "        else:\n",
    "            print(f\"âš  Model weights not found at {Config.model_path}. Using random initialization (for debugging only).\")\n",
    "            \n",
    "        model.to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None\n",
    "        \n",
    "    print(\"âœ“ Model and tokenizer ready\\n\")\n",
    "    \n",
    "    # Prepare test dataloader\n",
    "    test_dataset = QuestDataset(test_df, tokenizer, mode=\"test\")\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=Config.batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=Config.num_workers\n",
    "    )\n",
    "    \n",
    "    # Generate predictions\n",
    "    final_preds = generate_predictions(model, test_loader, device)\n",
    "    \n",
    "    # Post-processing\n",
    "    print(\"\\nApplying post-processing...\")\n",
    "    if os.path.exists(Config.train_csv):\n",
    "        train_df = pd.read_csv(Config.train_csv)\n",
    "        bins_dict = get_valid_bins(train_df, Config.target_cols)\n",
    "        final_preds = snap_predictions(final_preds, Config.target_cols, bins_dict)\n",
    "        print(\"âœ“ Predictions snapped to valid values\")\n",
    "\n",
    "        epsilon = 1e-6\n",
    "        noise = np.random.uniform(-epsilon, epsilon, final_preds.shape)\n",
    "        final_preds = final_preds + noise\n",
    "        final_preds = np.clip(final_preds, 0.0, 1.0)\n",
    "        print(\"âœ“ Added epsilon noise to prevent constant columns\")\n",
    "    else:\n",
    "        print(\"âš  train.csv not found, skipping post-processing\")\n",
    "\n",
    "    # Create submission\n",
    "    print(\"\\nCreating submission file...\")\n",
    "    submission = pd.DataFrame(final_preds, columns=Config.target_cols)\n",
    "    submission['qa_id'] = test_df['qa_id'].values\n",
    "    submission = submission[['qa_id'] + Config.target_cols]\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "    print(f\"âœ“ Submission saved to submission.csv\")\n",
    "            \n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbdd5a99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T18:10:13.857485Z",
     "iopub.status.busy": "2025-11-24T18:10:13.857155Z",
     "iopub.status.idle": "2025-11-24T18:10:39.871457Z",
     "shell.execute_reply": "2025-11-24T18:10:39.870481Z",
     "shell.execute_reply.started": "2025-11-24T18:10:13.857468Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Loading test data...\n",
      "Test samples: 476\n",
      "\n",
      "Loading model and tokenizer...\n",
      "âœ“ Loaded weights from /kaggle/input/quest-finetuned-weights/best-deberta-v3-base-ft.pth\n",
      "âœ“ Model and tokenizer ready\n",
      "\n",
      "Generating predictions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db44bd8f65934cd5b30765151b1daaf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Inference:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying post-processing...\n",
      "âœ“ Predictions snapped to valid values\n",
      "\n",
      "Creating submission file...\n",
      "âœ“ Submission saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 7. Run Inference\n",
    "# ==========================================\n",
    "submission = inference_pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 828965,
     "isSourceIdPinned": false,
     "sourceId": 7968,
     "sourceType": "competition"
    },
    {
     "datasetId": 8825661,
     "sourceId": 13854530,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8825600,
     "sourceId": 13854451,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
