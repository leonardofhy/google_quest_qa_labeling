{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Google QUEST Q&A Labeling - Inference & Submission\n",
                "\n",
                "This notebook performs inference using trained DeBERTa-v3-Large models and generates a submission file.\n",
                "It implements **LightGBM Stacking** (Path A) if OOF predictions are available, otherwise falls back to simple averaging.\n",
                "\n",
                "**Key Features:**\n",
                "- Offline compatible (no internet required)\n",
                "- 5-Fold Ensemble\n",
                "- Attention Pooling (matching training architecture)\n",
                "- LightGBM Stacking Post-Processing (with CV)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "\n",
                "# 1. Setup for Offline Environment\n",
                "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
                "sys.modules[\"tensorflow\"] = None # Prevent tensorflow import to avoid protobuf conflicts\n",
                "\n",
                "import gc\n",
                "import json\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
                "from tqdm.auto import tqdm\n",
                "import lightgbm as lgb\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fc676093",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# Configuration\n",
                "# ==========================================\n",
                "class Config:\n",
                "    # Paths (Adjust these for Kaggle)\n",
                "    model_name = \"/kaggle/input/deberta-v3-large-offline\" # Path to offline tokenizer/config\n",
                "    model_dir = \"/kaggle/input/deberta-v3-large-quest-qa-5fold\" # Path to saved weights\n",
                "    train_csv = \"/kaggle/input/google-quest-challenge/train.csv\"\n",
                "    test_csv = \"/kaggle/input/google-quest-challenge/test.csv\"\n",
                "    sample_submission = \"/kaggle/input/google-quest-challenge/sample_submission.csv\"\n",
                "    output_dir = \"/kaggle/working\" # Explicit output directory\n",
                "    \n",
                "    # Model Config\n",
                "    max_len = 1024 # Matching training config\n",
                "    batch_size = 8\n",
                "    num_workers = 4\n",
                "    seed = 42\n",
                "    n_folds = 5\n",
                "    \n",
                "    # Target Columns\n",
                "    target_cols = [\n",
                "        'question_asker_intent_understanding', 'question_body_critical', 'question_conversational',\n",
                "        'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer',\n",
                "        'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent',\n",
                "        'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice',\n",
                "        'question_type_compare', 'question_type_consequence', 'question_type_definition',\n",
                "        'question_type_entity', 'question_type_instructions', 'question_type_procedure',\n",
                "        'question_type_reason_explanation', 'question_type_spelling', 'question_well_written',\n",
                "        'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n",
                "        'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure',\n",
                "        'answer_type_reason_explanation', 'answer_well_written'\n",
                "    ]\n",
                "\n",
                "def seed_everything(seed=42):\n",
                "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    torch.cuda.manual_seed(seed)\n",
                "    torch.backends.cudnn.deterministic = True\n",
                "\n",
                "seed_everything(Config.seed)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6e857bf8",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# Dataset Class\n",
                "# ==========================================\n",
                "class QuestDataset(Dataset):\n",
                "    def __init__(self, df, tokenizer, max_len=512, mode='test'):\n",
                "        self.df = df\n",
                "        self.tokenizer = tokenizer\n",
                "        self.max_len = max_len\n",
                "        self.mode = mode\n",
                "        \n",
                "        self.titles = df['question_title'].values\n",
                "        self.bodies = df['question_body'].values\n",
                "        self.answers = df['answer'].values\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.df)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        title = str(self.titles[idx])\n",
                "        body = str(self.bodies[idx])\n",
                "        answer = str(self.answers[idx])\n",
                "        \n",
                "        # [CLS] title [SEP] body [SEP] answer [SEP]\n",
                "        q_text = title + \" \" + self.tokenizer.sep_token + \" \" + body\n",
                "        a_text = answer\n",
                "        \n",
                "        q_tokens = self.tokenizer.tokenize(q_text)\n",
                "        a_tokens = self.tokenizer.tokenize(a_text)\n",
                "        \n",
                "        # Head+Tail Truncation Logic\n",
                "        budget = self.max_len - 3\n",
                "        if len(q_tokens) + len(a_tokens) > budget:\n",
                "            half = budget // 2\n",
                "            if len(a_tokens) > half and len(q_tokens) > half:\n",
                "                a_tokens = a_tokens[:half]\n",
                "                q_tokens = q_tokens[:budget - len(a_tokens)]\n",
                "            elif len(a_tokens) <= half:\n",
                "                q_tokens = q_tokens[:budget - len(a_tokens)]\n",
                "            else:\n",
                "                a_tokens = a_tokens[:budget - len(q_tokens)]\n",
                "                \n",
                "        ids = [self.tokenizer.cls_token_id] + \\\n",
                "              self.tokenizer.convert_tokens_to_ids(q_tokens) + \\\n",
                "              [self.tokenizer.sep_token_id] + \\\n",
                "              self.tokenizer.convert_tokens_to_ids(a_tokens) + \\\n",
                "              [self.tokenizer.sep_token_id]\n",
                "              \n",
                "        mask = [1] * len(ids)\n",
                "        padding_len = self.max_len - len(ids)\n",
                "        ids = ids + [self.tokenizer.pad_token_id] * padding_len\n",
                "        mask = mask + [0] * padding_len\n",
                "        \n",
                "        return {\n",
                "            'input_ids': torch.tensor(ids, dtype=torch.long),\n",
                "            'attention_mask': torch.tensor(mask, dtype=torch.long)\n",
                "        }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6d642723",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# Model Class (Must match training!)\n",
                "# ==========================================\n",
                "class AttentionPooling(nn.Module):\n",
                "    def __init__(self, hidden_size):\n",
                "        super().__init__()\n",
                "        self.attention = nn.Sequential(\n",
                "            nn.Linear(hidden_size, hidden_size),\n",
                "            nn.Tanh(),\n",
                "            nn.Linear(hidden_size, 1),\n",
                "        )\n",
                "\n",
                "    def forward(self, hidden_states, attention_mask):\n",
                "        attention_scores = self.attention(hidden_states).squeeze(-1)\n",
                "        attention_scores = attention_scores.masked_fill(attention_mask == 0, float('-inf'))\n",
                "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
                "        pooled = torch.bmm(attention_weights.unsqueeze(1), hidden_states).squeeze(1)\n",
                "        return pooled\n",
                "\n",
                "class WeightedLayerPooling(nn.Module):\n",
                "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n",
                "        super(WeightedLayerPooling, self).__init__()\n",
                "        self.layer_start = layer_start\n",
                "        self.num_hidden_layers = num_hidden_layers\n",
                "        self.layer_weights = layer_weights if layer_weights is not None else nn.Parameter(\n",
                "            torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n",
                "        )\n",
                "\n",
                "    def forward(self, all_hidden_states):\n",
                "        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n",
                "        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n",
                "        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n",
                "        return weighted_average\n",
                "\n",
                "class Model(nn.Module):\n",
                "    def __init__(self, model_name):\n",
                "        super().__init__()\n",
                "        self.config = AutoConfig.from_pretrained(model_name)\n",
                "        self.config.output_hidden_states = True\n",
                "        self.bert = AutoModel.from_pretrained(model_name, config=self.config)\n",
                "        \n",
                "        # Weighted Layer Pooling\n",
                "        self.pooler = WeightedLayerPooling(\n",
                "            self.config.num_hidden_layers, \n",
                "            layer_start=4, \n",
                "            layer_weights=None\n",
                "        )\n",
                "        \n",
                "        # Attention Pooling\n",
                "        self.attention_pool = AttentionPooling(self.config.hidden_size)\n",
                "        \n",
                "        # Multi-Sample Dropout\n",
                "        self.dropouts = nn.ModuleList([nn.Dropout(0.1) for _ in range(5)])\n",
                "        self.linear = nn.Linear(self.config.hidden_size, 30)\n",
                "\n",
                "    def forward(self, input_ids, attention_mask):\n",
                "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
                "        all_hidden_states = torch.stack(outputs.hidden_states)\n",
                "        \n",
                "        # Weighted Layer Pooling\n",
                "        weighted_pooling_embeddings = self.pooler(all_hidden_states)\n",
                "        \n",
                "        # Attention Pooling\n",
                "        pooled_output = self.attention_pool(weighted_pooling_embeddings, attention_mask)\n",
                "        \n",
                "        # Multi-Sample Dropout\n",
                "        for i, dropout in enumerate(self.dropouts):\n",
                "            if i == 0:\n",
                "                x = self.linear(dropout(pooled_output))\n",
                "            else:\n",
                "                x += self.linear(dropout(pooled_output))\n",
                "        \n",
                "        x = x / len(self.dropouts)\n",
                "        return x"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eb004380",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# Inference Loop\n",
                "# ==========================================\n",
                "def inference_fn(model, dataloader, device):\n",
                "    model.eval()\n",
                "    preds = []\n",
                "    \n",
                "    for batch in tqdm(dataloader, desc=\"Inference\"):\n",
                "        input_ids = batch['input_ids'].to(device)\n",
                "        mask = batch['attention_mask'].to(device)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            output = model(input_ids, mask)\n",
                "        \n",
                "        preds.append(torch.sigmoid(output).cpu().numpy())\n",
                "        \n",
                "    return np.concatenate(preds)\n",
                "\n",
                "def run_inference():\n",
                "    # Load Data\n",
                "    test_df = pd.read_csv(Config.test_csv)\n",
                "    tokenizer = AutoTokenizer.from_pretrained(Config.model_name)\n",
                "    test_dataset = QuestDataset(test_df, tokenizer, max_len=Config.max_len)\n",
                "    test_loader = DataLoader(test_dataset, batch_size=Config.batch_size, shuffle=False, num_workers=Config.num_workers)\n",
                "    \n",
                "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "    fold_preds = []\n",
                "    \n",
                "    for fold in range(Config.n_folds):\n",
                "        print(f\"\\nRunning Fold {fold}...\")\n",
                "        \n",
                "        # CORRECTED PATH: best_model/qa_model_fold{fold}_best.bin\n",
                "        model_path = os.path.join(Config.model_dir, \"best_model\", f\"qa_model_fold{fold}_best.bin\")\n",
                "        \n",
                "        if not os.path.exists(model_path):\n",
                "            print(f\"âš ï¸ Model not found: {model_path}. Skipping.\")\n",
                "            continue\n",
                "            \n",
                "        model = Model(Config.model_name)\n",
                "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
                "        model.to(device)\n",
                "        \n",
                "        preds = inference_fn(model, test_loader, device)\n",
                "        fold_preds.append(preds)\n",
                "        \n",
                "        del model, preds\n",
                "        torch.cuda.empty_cache()\n",
                "        gc.collect()\n",
                "        \n",
                "    return fold_preds, test_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b4de5659",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==========================================\n",
                "# Main Execution\n",
                "# ==========================================\n",
                "fold_preds, test_df = run_inference()\n",
                "\n",
                "# Check for OOF predictions for LightGBM Stacking\n",
                "oof_path = os.path.join(Config.model_dir, \"oof_preds.npy\")\n",
                "fold_ids_path = os.path.join(Config.model_dir, \"fold_ids.json\")\n",
                "use_stacking = os.path.exists(oof_path) and os.path.exists(Config.train_csv)\n",
                "\n",
                "if use_stacking:\n",
                "    print(\"\\nðŸš€ Found OOF predictions! Running LightGBM Stacking...\")\n",
                "    \n",
                "    # Load Train Data and OOFs\n",
                "    train_df = pd.read_csv(Config.train_csv)\n",
                "    oof_preds = np.load(oof_path) \n",
                "    \n",
                "    # Load Fold IDs if available\n",
                "    fold_ids = None\n",
                "    if os.path.exists(fold_ids_path):\n",
                "        with open(fold_ids_path, \"r\") as f:\n",
                "            fold_ids = json.load(f)\n",
                "        print(f\"âœ… Loaded Fold IDs for CV.\")\n",
                "    else:\n",
                "        print(f\"âš ï¸ Fold IDs not found. Will use random split for Early Stopping.\")\n",
                "    \n",
                "    # Prepare Features\n",
                "    if len(oof_preds.shape) == 3:\n",
                "         print(f\"âš ï¸ OOF shape {oof_preds.shape} implies multi-epoch. Using mean of OOFs as features.\")\n",
                "         x_train_all = np.mean(oof_preds, axis=0) # (n_samples, 30)\n",
                "    else:\n",
                "        x_train_all = oof_preds\n",
                "        \n",
                "    # Test Features\n",
                "    x_test_all = np.mean(fold_preds, axis=0) # (n_test, 30)\n",
                "    \n",
                "    final_preds = np.zeros_like(x_test_all)\n",
                "    \n",
                "    lgb_params = {\n",
                "        \"boosting_type\": \"gbdt\",\n",
                "        \"objective\": \"rmse\",\n",
                "        \"learning_rate\": 0.1,\n",
                "        \"max_depth\": 3,\n",
                "        \"seed\": 71,\n",
                "        \"verbose\": -1,\n",
                "        \"n_jobs\": 4\n",
                "    }\n",
                "    \n",
                "    for i, col in enumerate(Config.target_cols):\n",
                "        y_train = train_df[col].values\n",
                "        \n",
                "        # Use Fold IDs for CV if available\n",
                "        if fold_ids:\n",
                "            test_preds_col = np.zeros(len(test_df))\n",
                "            # fold_ids is list of [train_idx, val_idx]\n",
                "            for trn_idx, val_idx in fold_ids:\n",
                "                x_tr, x_val = x_train_all[trn_idx], x_train_all[val_idx]\n",
                "                y_tr, y_val = y_train[trn_idx], y_train[val_idx]\n",
                "                \n",
                "                d_train = lgb.Dataset(x_tr, label=y_tr)\n",
                "                d_valid = lgb.Dataset(x_val, label=y_val)\n",
                "                \n",
                "                model = lgb.train(\n",
                "                    lgb_params, \n",
                "                    d_train, \n",
                "                    num_boost_round=1000,\n",
                "                    valid_sets=[d_valid],\n",
                "                    callbacks=[lgb.early_stopping(stopping_rounds=30, verbose=False)]\n",
                "                )\n",
                "                \n",
                "                test_preds_col += model.predict(x_test_all) / len(fold_ids)\n",
                "            \n",
                "            final_preds[:, i] = test_preds_col\n",
                "            \n",
                "        else:\n",
                "            # Fallback to simple split\n",
                "            from sklearn.model_selection import train_test_split\n",
                "            x_tr, x_val, y_tr, y_val = train_test_split(x_train_all, y_train, test_size=0.2, random_state=42)\n",
                "            \n",
                "            d_train = lgb.Dataset(x_tr, label=y_tr)\n",
                "            d_valid = lgb.Dataset(x_val, label=y_val)\n",
                "            \n",
                "            model = lgb.train(\n",
                "                lgb_params, \n",
                "                d_train, \n",
                "                num_boost_round=1000,\n",
                "                valid_sets=[d_valid],\n",
                "                callbacks=[lgb.early_stopping(stopping_rounds=30, verbose=False)]\n",
                "            )\n",
                "            \n",
                "            final_preds[:, i] = model.predict(x_test_all)\n",
                "        \n",
                "    print(\"âœ… LightGBM Stacking Completed.\")\n",
                "else:\n",
                "    print(\"\\nâ„¹ï¸ OOF predictions not found. Using Simple Averaging.\")\n",
                "    final_preds = np.mean(fold_preds, axis=0)\n",
                "\n",
                "# Create Submission\n",
                "sub = pd.DataFrame(final_preds, columns=Config.target_cols)\n",
                "sub['qa_id'] = test_df['qa_id']\n",
                "sub = sub[['qa_id'] + Config.target_cols]\n",
                "output_path = os.path.join(Config.output_dir, \"submission.csv\")\n",
                "sub.to_csv(output_path, index=False)\n",
                "print(f\"\\nâœ… submission.csv saved to {output_path}!\")\n",
                "sub.head()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
