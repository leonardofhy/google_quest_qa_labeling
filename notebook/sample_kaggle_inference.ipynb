{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7968,"databundleVersionId":828965,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":13854451,"sourceType":"datasetVersion","datasetId":8825600},{"sourceId":13859824,"sourceType":"datasetVersion","datasetId":8829497},{"sourceId":13861509,"sourceType":"datasetVersion","datasetId":8826659}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport os\n\n# 1. è¨­ç½® Protobuf ä½¿ç”¨ Python å¯¦ç¾\nos.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n\n# 2. é—œéµä¿®å¾©ï¼šå±è”½ TensorFlow\n# é€™æœƒé˜²æ­¢ transformers å°Žå…¥ tensorflowï¼Œå¾žè€Œé¿å… protobuf ç‰ˆæœ¬è¡çª\nsys.modules[\"tensorflow\"] = None ","metadata":{"execution":{"iopub.status.busy":"2025-11-24T20:48:25.860788Z","iopub.execute_input":"2025-11-24T20:48:25.861039Z","iopub.status.idle":"2025-11-24T20:48:25.865000Z","shell.execute_reply.started":"2025-11-24T20:48:25.861022Z","shell.execute_reply":"2025-11-24T20:48:25.864188Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom tqdm.auto import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T20:48:25.866174Z","iopub.execute_input":"2025-11-24T20:48:25.866428Z","iopub.status.idle":"2025-11-24T20:48:25.877694Z","shell.execute_reply.started":"2025-11-24T20:48:25.866403Z","shell.execute_reply":"2025-11-24T20:48:25.876971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# éæ­· input ç›®éŒ„ï¼ŒæŸ¥çœ‹æª”æ¡ˆçµæ§‹\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2025-11-24T20:48:25.880776Z","iopub.execute_input":"2025-11-24T20:48:25.881061Z","iopub.status.idle":"2025-11-24T20:48:25.892234Z","shell.execute_reply.started":"2025-11-24T20:48:25.881036Z","shell.execute_reply":"2025-11-24T20:48:25.891478Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# 1. Configuration\n# ==========================================\nclass Config:\n    \"\"\"Inference configuration\"\"\"\n    model_name = \"/kaggle/input/deberta-v3-base-offline\"\n    max_len = 512\n    batch_size = 16  # Can increase batch size for inference\n    num_workers = 2\n    seed = 42\n    \n    # Paths - Updated for Kaggle environment\n    train_csv = \"/kaggle/input/google-quest-challenge/train.csv\" \n    test_csv = \"/kaggle/input/google-quest-challenge/test.csv\"\n    sample_submission_csv = \"/kaggle/input/google-quest-challenge/sample_submission.csv\"\n    \n    # Path to the trained model weights\n    model_path = \"/kaggle/input/quest-finetuned-weights/best_deberta-v3-base-2.pth\"\n    \n    target_cols = [\n        'question_asker_intent_understanding', 'question_body_critical', 'question_conversational',\n        'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer',\n        'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent',\n        'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice',\n        'question_type_compare', 'question_type_consequence', 'question_type_definition',\n        'question_type_entity', 'question_type_instructions', 'question_type_procedure',\n        'question_type_reason_explanation', 'question_type_spelling', 'question_well_written',\n        'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n        'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure',\n        'answer_type_reason_explanation', 'answer_well_written'\n    ]\n\ndef seed_everything(seed=42):\n    \"\"\"Set random seeds for reproducibility\"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything(Config.seed)\n\n# Display configuration\nprint(\"âœ… Configuration loaded successfully!\\n\")\nprint(f\"ðŸ¤– Model: {Config.model_name}\")\nprint(f\"ðŸ“ Max Length: {Config.max_len}\")\nprint(f\"ðŸ“¦ Batch Size: {Config.batch_size}\")\nprint(f\"ðŸŽ² Random Seed: {Config.seed}\")\nprint(f\"\\nðŸ“Š Number of target labels: {len(Config.target_cols)}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-11-24T20:48:25.901292Z","iopub.execute_input":"2025-11-24T20:48:25.901484Z","iopub.status.idle":"2025-11-24T20:48:25.910076Z","shell.execute_reply.started":"2025-11-24T20:48:25.901470Z","shell.execute_reply":"2025-11-24T20:48:25.909352Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# 2. Dataset Class\n# ==========================================\nclass QuestDataset(Dataset):\n    \"\"\"Custom dataset for Q&A labeling task\"\"\"\n    \n    def __init__(self, df, tokenizer, max_len=512, mode=\"test\"):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.mode = mode\n        \n        self.titles = df['question_title'].values\n        self.bodies = df['question_body'].values\n        self.answers = df['answer'].values\n        \n        if self.mode != \"test\":\n            self.targets = df[Config.target_cols].values\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        title = str(self.titles[idx])\n        body = str(self.bodies[idx])\n        answer = str(self.answers[idx])\n        \n        # Combine question parts\n        q_text = title + \" \" + self.tokenizer.sep_token + \" \" + body\n        a_text = answer\n        \n        # Tokenize\n        q_tokens = self.tokenizer.tokenize(q_text)\n        a_tokens = self.tokenizer.tokenize(a_text)\n        \n        # Dynamic truncation with budget awareness\n        budget = self.max_len - 3  # [CLS], [SEP], [SEP]\n        if len(q_tokens) + len(a_tokens) > budget:\n            half = budget // 2\n            if len(a_tokens) > half and len(q_tokens) > half:\n                a_tokens = a_tokens[:half]\n                q_tokens = q_tokens[:budget - len(a_tokens)]\n            elif len(a_tokens) <= half:\n                q_tokens = q_tokens[:budget - len(a_tokens)]\n            else:\n                a_tokens = a_tokens[:budget - len(q_tokens)]\n                \n        # Build input IDs\n        ids = [self.tokenizer.cls_token_id] + \\\n              self.tokenizer.convert_tokens_to_ids(q_tokens) + \\\n              [self.tokenizer.sep_token_id] + \\\n              self.tokenizer.convert_tokens_to_ids(a_tokens) + \\\n              [self.tokenizer.sep_token_id]\n              \n        mask = [1] * len(ids)\n        padding_len = self.max_len - len(ids)\n        ids = ids + [self.tokenizer.pad_token_id] * padding_len\n        mask = mask + [0] * padding_len\n        \n        output = {\n            'input_ids': torch.tensor(ids, dtype=torch.long),\n            'attention_mask': torch.tensor(mask, dtype=torch.long)\n        }\n        \n        if self.mode != \"test\":\n            output['labels'] = torch.tensor(self.targets[idx], dtype=torch.float)\n            \n        return output","metadata":{"execution":{"iopub.status.busy":"2025-11-24T20:48:25.911374Z","iopub.execute_input":"2025-11-24T20:48:25.911842Z","iopub.status.idle":"2025-11-24T20:48:25.921544Z","shell.execute_reply.started":"2025-11-24T20:48:25.911826Z","shell.execute_reply":"2025-11-24T20:48:25.920842Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# 3. Model Class\n# ==========================================\nclass QuestDebertaModel(nn.Module):\n    \"\"\"DeBERTa model with weighted layer pooling and multi-sample dropout\"\"\"\n    \n    def __init__(self, model_name=Config.model_name, num_labels=30):\n        super().__init__()\n        self.config = AutoConfig.from_pretrained(model_name)\n        self.config.output_hidden_states = True\n        self.model = AutoModel.from_pretrained(model_name, config=self.config)\n        \n        # Weighted layer pooling\n        n_weights = self.config.num_hidden_layers + 1\n        weights_init = torch.zeros(n_weights).float()\n        weights_init.data[:-1] = -3\n        self.layer_weights = nn.Parameter(weights_init)\n        \n        # Multi-sample dropout\n        self.dropouts = nn.ModuleList([nn.Dropout(0.5) for _ in range(5)])\n        self.fc = nn.Linear(self.config.hidden_size, num_labels)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        hidden_states = outputs.hidden_states \n        \n        # Stack [CLS] tokens\n        cls_outputs = torch.stack([layer[:, 0, :] for layer in hidden_states], dim=1)\n        \n        # Weighted sum\n        weights = torch.softmax(self.layer_weights, dim=0).view(1, -1, 1)\n        weighted_cls = (weights * cls_outputs).sum(dim=1)\n        \n        # Multi-sample dropout\n        logits_list = []\n        for dropout in self.dropouts:\n            logits_list.append(self.fc(dropout(weighted_cls)))\n        avg_logits = torch.mean(torch.stack(logits_list, dim=0), dim=0)\n        \n        return self.sigmoid(avg_logits)","metadata":{"execution":{"iopub.status.busy":"2025-11-24T20:48:26.017222Z","iopub.execute_input":"2025-11-24T20:48:26.017398Z","iopub.status.idle":"2025-11-24T20:48:26.024273Z","shell.execute_reply.started":"2025-11-24T20:48:26.017385Z","shell.execute_reply":"2025-11-24T20:48:26.023447Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# 4. Post-processing Utilities\n# ==========================================\ndef get_valid_bins(train_df, target_cols):\n    \"\"\"Extract valid bin values from training data\"\"\"\n    unique_bins = {}\n    for col in target_cols:\n        bins = np.sort(train_df[col].unique())\n        unique_bins[col] = bins\n    return unique_bins\n\ndef snap_predictions(predictions, target_cols, unique_bins):\n    \"\"\"Round predictions to nearest valid values from training set\"\"\"\n    snapped = predictions.copy()\n    for i, col in enumerate(target_cols):\n        valid_vals = unique_bins[col]\n        \n        # Only snap sparse columns (< 100 unique values)\n        if len(valid_vals) > 100:\n            continue\n            \n        col_preds = snapped[:, i].reshape(-1, 1)\n        diffs = np.abs(col_preds - valid_vals.reshape(1, -1))\n        min_indices = np.argmin(diffs, axis=1)\n        snapped[:, i] = valid_vals[min_indices]\n    return snapped","metadata":{"execution":{"iopub.status.busy":"2025-11-24T20:48:26.025586Z","iopub.execute_input":"2025-11-24T20:48:26.025882Z","iopub.status.idle":"2025-11-24T20:48:26.037779Z","shell.execute_reply.started":"2025-11-24T20:48:26.025855Z","shell.execute_reply":"2025-11-24T20:48:26.037068Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# 5. Inference Pipeline\n# ==========================================\n@torch.no_grad()\ndef generate_predictions(model, test_loader, device):\n    \"\"\"Generate predictions on test set\"\"\"\n    model.eval()\n    all_preds = []\n    \n    print(\"Generating predictions...\")\n    for batch in tqdm(test_loader, desc=\"Inference\"):\n        input_ids = batch['input_ids'].to(device)\n        mask = batch['attention_mask'].to(device)\n        \n        outputs = model(input_ids, mask)\n        all_preds.append(outputs.cpu().numpy())\n            \n    return np.concatenate(all_preds)\n\ndef inference_pipeline():\n    \"\"\"Complete inference pipeline\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\\n\")\n    \n    # Load test data\n    print(\"Loading test data...\")\n    if not os.path.exists(Config.test_csv):\n        print(f\"Error: Test file not found at {Config.test_csv}\")\n        return None\n        \n    test_df = pd.read_csv(Config.test_csv)\n    print(f\"Test samples: {len(test_df)}\\n\")\n    \n    # Load tokenizer and model\n    print(\"Loading model and tokenizer...\")\n    try:\n        tokenizer = AutoTokenizer.from_pretrained(Config.model_name)\n        model = QuestDebertaModel()\n        \n        if os.path.exists(Config.model_path):\n            state_dict = torch.load(Config.model_path, map_location=device)\n            model.load_state_dict(state_dict)\n            print(f\"âœ“ Loaded weights from {Config.model_path}\")\n        else:\n            print(f\"âš  Model weights not found at {Config.model_path}. Using random initialization (for debugging only).\")\n            \n        model.to(device)\n    except Exception as e:\n        print(f\"Error loading model: {e}\")\n        return None\n        \n    print(\"âœ“ Model and tokenizer ready\\n\")\n    \n    # Prepare test dataloader\n    test_dataset = QuestDataset(test_df, tokenizer, mode=\"test\")\n    test_loader = DataLoader(\n        test_dataset, \n        batch_size=Config.batch_size, \n        shuffle=False, \n        num_workers=Config.num_workers\n    )\n    \n    # Generate predictions\n    final_preds = generate_predictions(model, test_loader, device)\n    \n    # Post-processing\n    print(\"\\nApplying post-processing...\")\n    if os.path.exists(Config.train_csv):\n        train_df = pd.read_csv(Config.train_csv)\n        bins_dict = get_valid_bins(train_df, Config.target_cols)\n        final_preds = snap_predictions(final_preds, Config.target_cols, bins_dict)\n        print(\"âœ“ Predictions snapped to valid values\")\n\n        epsilon = 1e-6\n        noise = np.random.uniform(-epsilon, epsilon, final_preds.shape)\n        final_preds = final_preds + noise\n        final_preds = np.clip(final_preds, 0.0, 1.0)\n        print(\"âœ“ Added epsilon noise to prevent constant columns\")\n    else:\n        print(\"âš  train.csv not found, skipping post-processing\")\n\n    # Create submission\n    print(\"\\nCreating submission file...\")\n    submission = pd.DataFrame(final_preds, columns=Config.target_cols)\n    submission['qa_id'] = test_df['qa_id'].values\n    submission = submission[['qa_id'] + Config.target_cols]\n    submission.to_csv(\"submission.csv\", index=False)\n    print(f\"âœ“ Submission saved to submission.csv\")\n            \n    return submission","metadata":{"execution":{"iopub.status.busy":"2025-11-24T20:48:26.040737Z","iopub.execute_input":"2025-11-24T20:48:26.040947Z","iopub.status.idle":"2025-11-24T20:48:36.180655Z","shell.execute_reply.started":"2025-11-24T20:48:26.040933Z","shell.execute_reply":"2025-11-24T20:48:36.179688Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# 7. Run Inference\n# ==========================================\nsubmission = inference_pipeline()\n","metadata":{"execution":{"iopub.status.busy":"2025-11-24T20:48:36.181830Z","iopub.execute_input":"2025-11-24T20:48:36.182060Z","iopub.status.idle":"2025-11-24T20:49:06.666588Z","shell.execute_reply.started":"2025-11-24T20:48:36.182043Z","shell.execute_reply":"2025-11-24T20:49:06.665749Z"},"trusted":true},"outputs":[],"execution_count":null}]}